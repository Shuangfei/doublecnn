train_epochs: 10
dropout: 1
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 20
lr: 1.0
std_p: 0.1
train_on_valid: 1
0	0.3234	0.3168	0.3168
1	0.3131	0.3091	0.3091
2	0.2946	0.2876	0.2876
3	0.2775	0.2259	0.2259
4	0.2625	0.2478	0.2478
5	0.2441	0.2578	0.2578
6	0.2246	0.2494	0.2494
7	0.2174	0.2435	0.2435
8	0.2058	0.2597	0.2597
9	0.1895	0.2152	0.2152
0.1895 0.2152 0.2152
train_epochs: 20
dropout: 1
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 20
lr: 1.0
std_p: 0.1
train_on_valid: 1
0	0.0466	0.1936	0.1936
1	0.0389	0.2006	0.2006
2	0.0417	0.1891	0.1891
3	0.0420	0.2073	0.2073
4	0.0321	0.1802	0.1802
5	0.0326	0.1940	0.1940
6	0.0285	0.1836	0.1836
7	0.0283	0.1782	0.1782
8	0.0267	0.2129	0.2129
9	0.0218	0.1774	0.1774
10	0.0233	0.1671	0.1671
11	0.0198	0.1924	0.1924
12	0.0234	0.1736	0.1736
13	0.0182	0.1725	0.1725
14	0.0197	0.1724	0.1724
15	0.0186	0.1747	0.1747
16	0.0199	0.1701	0.1701
17	0.0172	0.1741	0.1741
18	0.0157	0.1728	0.1728
19	0.0204	0.1953	0.1953
0.0233 0.1671 0.1671
train_epochs: 20
dropout: 1
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 20
lr: 1.0
std_p: 0.1
train_on_valid: 1
0	0.6848	0.4984	0.4984
1	0.3934	0.3473	0.3473
2	0.2842	0.2805	0.2805
3	0.2246	0.2391	0.2391
4	0.1858	0.2380	0.2380
5	0.1624	0.1930	0.1930
6	0.1427	0.1867	0.1867
7	0.1240	0.1800	0.1800
8	0.1109	0.1880	0.1880
9	0.0990	0.1777	0.1777
10	0.0874	0.1675	0.1675
11	0.0787	0.1707	0.1707
12	0.0676	0.1671	0.1671
13	0.0601	0.1702	0.1702
14	0.0521	0.1705	0.1705
15	0.0464	0.1745	0.1745
16	0.0409	0.1686	0.1686
17	0.0362	0.1691	0.1691
18	0.0331	0.1793	0.1793
19	0.0274	0.1635	0.1635
0.0274 0.1635 0.1635
train_epochs: 20
dropout: 1
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 20
lr: 1.0
std_p: 0.1
train_on_valid: 1
0	0.7192	0.5720	0.5720
1	0.4420	0.3618	0.3618
2	0.3267	0.3064	0.3064
3	0.2690	0.2701	0.2701
4	0.2232	0.2471	0.2471
5	0.1890	0.2382	0.2382
6	0.1603	0.2444	0.2444
7	0.1424	0.2156	0.2156
8	0.1238	0.2123	0.2123
9	0.1035	0.2150	0.2150
10	0.0899	0.2019	0.2019
11	0.0809	0.2044	0.2044
12	0.0669	0.1973	0.1973
13	0.0561	0.1968	0.1968
14	0.0491	0.2010	0.2010
15	0.0448	0.1978	0.1978
16	0.0374	0.1985	0.1985
17	0.0306	0.1943	0.1943
18	0.0259	0.2011	0.2011
19	0.0230	0.1852	0.1852
0.0230 0.1852 0.1852
train_epochs: 20
dropout: 1
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 20
lr: 1.0
std_p: 0.1
train_on_valid: 1
0	0.7115	0.5995	0.5995
1	0.5732	0.5281	0.5281
2	0.5068	0.4871	0.4871
3	0.4636	0.4559	0.4559
4	0.4281	0.4453	0.4453
5	0.4001	0.4261	0.4261
6	0.3823	0.3929	0.3929
7	0.3669	0.3986	0.3986
8	0.3524	0.3892	0.3892
9	0.3420	0.3756	0.3756
10	0.3306	0.3606	0.3606
11	0.3181	0.3568	0.3568
12	0.3111	0.3466	0.3466
13	0.3034	0.3513	0.3513
14	0.2965	0.3427	0.3427
15	0.2906	0.3346	0.3346
16	0.2873	0.3337	0.3337
17	0.2816	0.3290	0.3290
18	0.2777	0.3287	0.3287
19	0.2733	0.3412	0.3412
0.2777 0.3287 0.3287
train_epochs: 20
dropout: 1
save_model: model_saved.npz
filter_shape: [(128, 3, 3), (128, 3, 3), (128, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 20
lr: 1.0
std_p: 0.1
train_on_valid: 1
0	0.6482	0.5548	0.5548
1	0.4889	0.4486	0.4486
2	0.4097	0.3990	0.3990
3	0.3543	0.3730	0.3730
4	0.3148	0.3670	0.3670
5	0.2867	0.3677	0.3677
6	0.2593	0.3436	0.3436
7	0.2375	0.3321	0.3321
8	0.2145	0.3441	0.3441
9	0.1964	0.3262	0.3262
10	0.1770	0.3183	0.3183
11	0.1588	0.3270	0.3270
12	0.1473	0.3418	0.3418
13	0.1289	0.3334	0.3334
14	0.1147	0.3259	0.3259
15	0.1057	0.3393	0.3393
16	0.0955	0.3422	0.3422
17	0.0866	0.3332	0.3332
18	0.0718	0.3335	0.3335
19	0.0655	0.3399	0.3399
0.1770 0.3183 0.3183
train_epochs: 20
dropout: 1
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 20
lr: 1.0
std_p: 0.1
train_on_valid: 1
0	0.7375	0.6279	0.6279
1	0.6055	0.5751	0.5751
2	0.5368	0.5249	0.5249
3	0.4892	0.4825	0.4825
4	0.4444	0.4400	0.4400
5	0.4123	0.4183	0.4183
6	0.3830	0.4151	0.4151
7	0.3613	0.3864	0.3864
8	0.3460	0.3738	0.3738
9	0.3282	0.3646	0.3646
10	0.3121	0.3612	0.3612
11	0.2980	0.3583	0.3583
12	0.2854	0.3420	0.3420
13	0.2733	0.3345	0.3345
14	0.2648	0.3330	0.3330
15	0.2541	0.3387	0.3387
16	0.2461	0.3246	0.3246
17	0.2368	0.3227	0.3227
18	0.2297	0.3251	0.3251
19	0.2203	0.3265	0.3265
0.2368 0.3227 0.3227
train_epochs: 50
dropout: 1
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 20
lr: 1.0
std_p: 0.1
train_on_valid: 1
0	0.7630	0.6365	0.6365
1	0.5934	0.5475	0.5475
2	0.5005	0.4701	0.4701
3	0.4362	0.4141	0.4141
4	0.3938	0.3851	0.3851
5	0.3614	0.3915	0.3915
6	0.3298	0.3552	0.3552
7	0.3057	0.3351	0.3351
8	0.2888	0.2996	0.2996
9	0.2710	0.3194	0.3194
10	0.2594	0.3261	0.3261
11	0.2474	0.2855	0.2855
12	0.2367	0.2843	0.2843
13	0.2250	0.2807	0.2807
14	0.2182	0.2881	0.2881
15	0.2121	0.2644	0.2644
16	0.2030	0.2647	0.2647
17	0.1974	0.2785	0.2785
18	0.1896	0.2524	0.2524
19	0.1846	0.2579	0.2579
20	0.1767	0.2633	0.2633
21	0.1765	0.2555	0.2555
22	0.1702	0.2631	0.2631
23	0.1658	0.2484	0.2484
24	0.1623	0.2432	0.2432
25	0.1580	0.2443	0.2443
26	0.1540	0.2447	0.2447
27	0.1506	0.2554	0.2554
28	0.1453	0.2440	0.2440
29	0.1431	0.2421	0.2421
30	0.1423	0.2375	0.2375
31	0.1371	0.2412	0.2412
32	0.1360	0.2366	0.2366
33	0.1316	0.2406	0.2406
34	0.1279	0.2335	0.2335
35	0.1252	0.2327	0.2327
36	0.1237	0.2397	0.2397
37	0.1205	0.2395	0.2395
38	0.1211	0.2331	0.2331
39	0.1181	0.2468	0.2468
40	0.1154	0.2305	0.2305
41	0.1144	0.2327	0.2327
42	0.1115	0.2372	0.2372
43	0.1100	0.2325	0.2325
44	0.1093	0.2369	0.2369
45	0.1035	0.2381	0.2381
46	0.1042	0.2387	0.2387
47	0.1039	0.2270	0.2270
48	0.1012	0.2312	0.2312
49	0.0995	0.2289	0.2289
0.1039 0.2270 0.2270
train_epochs: 50
dropout: 1
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 20
lr: 1.0
std_p: 0.1
train_on_valid: 1
0	0.6576	0.4986	0.4986
1	0.5674	0.4677	0.4677
2	0.4925	0.3293	0.3293
3	0.4723	0.3568	0.3568
4	0.4479	0.3328	0.3328
5	0.4118	0.2764	0.2764
6	0.3799	0.2641	0.2641
7	0.3329	0.2435	0.2435
8	0.2965	0.2474	0.2474
9	0.2493	0.2179	0.2179
10	0.2145	0.2035	0.2035
11	0.1782	0.2112	0.2112
12	0.1580	0.2158	0.2158
13	0.1254	0.1967	0.1967
14	0.1129	0.1779	0.1779
15	0.1019	0.1948	0.1948
16	0.0864	0.1799	0.1799
17	0.0803	0.2008	0.2008
18	0.0720	0.1753	0.1753
19	0.0616	0.1691	0.1691
20	0.0601	0.1814	0.1814
21	0.0526	0.1894	0.1894
22	0.0477	0.1727	0.1727
23	0.0450	0.1713	0.1713
24	0.0441	0.1928	0.1928
25	0.0423	0.1723	0.1723
26	0.0370	0.1781	0.1781
27	0.0313	0.1669	0.1669
28	0.0352	0.1790	0.1790
29	0.0280	0.1781	0.1781
30	0.0271	0.1823	0.1823
31	0.0265	0.1722	0.1722
32	0.0224	0.1822	0.1822
33	0.0230	0.1756	0.1756
34	0.0221	0.1602	0.1602
35	0.0209	0.1726	0.1726
36	0.0187	0.1572	0.1572
37	0.0166	0.1566	0.1566
38	0.0161	0.1628	0.1628
39	0.0187	0.1640	0.1640
40	0.0168	0.1685	0.1685
41	0.0138	0.1787	0.1787
42	0.0127	0.1671	0.1671
43	0.0143	0.1668	0.1668
44	0.0115	0.1619	0.1619
45	0.0127	0.1615	0.1615
46	0.0096	0.1731	0.1731
47	0.0102	0.1667	0.1667
48	0.0108	0.1633	0.1633
49	0.0093	0.1646	0.1646
0.0166 0.1566 0.1566
train_epochs: 50
dropout: 1
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 20
lr: 1.0
std_p: 0.1
train_on_valid: 1
0	0.7086	0.6533	0.6533
1	0.5073	0.4035	0.4035
2	0.4008	0.3582	0.3582
3	0.3339	0.2940	0.2940
4	0.2854	0.2791	0.2791
5	0.2459	0.2477	0.2477
6	0.2203	0.2517	0.2517
7	0.2027	0.2649	0.2649
8	0.1821	0.2943	0.2943
9	0.1701	0.2159	0.2159
10	0.1567	0.2615	0.2615
11	0.1478	0.1884	0.1884
12	0.1381	0.1801	0.1801
13	0.1315	0.2091	0.2091
14	0.1215	0.1871	0.1871
15	0.1178	0.2045	0.2045
16	0.1095	0.1609	0.1609
17	0.1039	0.1727	0.1727
18	0.0983	0.1790	0.1790
19	0.0933	0.1800	0.1800
20	0.0897	0.1554	0.1554
21	0.0864	0.1630	0.1630
22	0.0815	0.1766	0.1766
23	0.0785	0.1638	0.1638
24	0.0728	0.1652	0.1652
25	0.0713	0.1535	0.1535
26	0.0677	0.1593	0.1593
27	0.0646	0.1722	0.1722
28	0.0610	0.1691	0.1691
29	0.0609	0.1621	0.1621
30	0.0551	0.1597	0.1597
31	0.0538	0.1554	0.1554
32	0.0505	0.1642	0.1642
33	0.0506	0.1599	0.1599
34	0.0500	0.1515	0.1515
35	0.0464	0.1439	0.1439
36	0.0438	0.1703	0.1703
37	0.0436	0.1634	0.1634
38	0.0407	0.1727	0.1727
39	0.0392	0.1546	0.1546
40	0.0380	0.1676	0.1676
41	0.0383	0.1556	0.1556
42	0.0353	0.1631	0.1631
43	0.0345	0.1543	0.1543
44	0.0315	0.1514	0.1514
45	0.0327	0.1628	0.1628
46	0.0311	0.1583	0.1583
47	0.0306	0.1523	0.1523
48	0.0274	0.1478	0.1478
49	0.0267	0.1610	0.1610
0.0464 0.1439 0.1439
train_epochs: 100
dropout: 1
save_model: model_saved.npz
filter_shape: [(96, 3, 3), (96, 3, 3), (96, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 15
lr: 1.0
std_p: 0.1
train_on_valid: 1
0	0.6606	0.4787	0.4787
1	0.4584	0.3937	0.3937
2	0.3626	0.3882	0.3882
3	0.3032	0.2525	0.2525
4	0.2537	0.2411	0.2411
5	0.2240	0.2375	0.2375
6	0.1947	0.2214	0.2214
7	0.1723	0.2072	0.2072
8	0.1537	0.1973	0.1973
9	0.1429	0.1933	0.1933
10	0.1225	0.2015	0.2015
11	0.1085	0.1726	0.1726
12	0.1022	0.1935	0.1935
13	0.0928	0.1839	0.1839
14	0.0788	0.1954	0.1954
15	0.0694	0.1654	0.1654
16	0.0641	0.1595	0.1595
17	0.0544	0.1765	0.1765
18	0.0483	0.1719	0.1719
19	0.0430	0.1778	0.1778
20	0.0439	0.1717	0.1717
21	0.0383	0.1603	0.1603
22	0.0310	0.1507	0.1507
23	0.0309	0.1630	0.1630
24	0.0279	0.1458	0.1458
25	0.0223	0.1588	0.1588
26	0.0209	0.1478	0.1478
27	0.0163	0.1417	0.1417
28	0.0153	0.1557	0.1557
29	0.0177	0.1557	0.1557
30	0.0151	0.1544	0.1544
31	0.0137	0.1506	0.1506
32	0.0138	0.1431	0.1431
33	0.0144	0.1463	0.1463
34	0.0125	0.1478	0.1478
35	0.0088	0.1481	0.1481
36	0.0084	0.1481	0.1481
37	0.0085	0.1387	0.1387
38	0.0078	0.1339	0.1339
39	0.0075	0.1358	0.1358
40	0.0056	0.1360	0.1360
41	0.0068	0.1401	0.1401
42	0.0062	0.1389	0.1389
43	0.0059	0.1436	0.1436
44	0.0051	0.1460	0.1460
45	0.0055	0.1353	0.1353
46	0.0041	0.1388	0.1388
47	0.0047	0.1411	0.1411
48	0.0046	0.1468	0.1468
49	0.0038	0.1403	0.1403
50	0.0044	0.1563	0.1563
51	0.0037	0.1398	0.1398
52	0.0029	0.1386	0.1386
53	0.0024	0.1353	0.1353
54	0.0024	0.1397	0.1397
55	0.0003	0.1252	0.1252
56	0.0000	0.1217	0.1217
57	0.0000	0.1234	0.1234
58	0.0000	0.1237	0.1237
59	0.0000	0.1249	0.1249
60	0.0000	0.1255	0.1255
61	0.0000	0.1249	0.1249
62	0.0000	0.1249	0.1249
63	0.0000	0.1229	0.1229
64	0.0000	0.1239	0.1239
65	0.0000	0.1228	0.1228
66	0.0000	0.1224	0.1224
67	0.0000	0.1236	0.1236
68	0.0000	0.1230	0.1230
69	0.0000	0.1243	0.1243
70	0.0000	0.1216	0.1216
71	0.0000	0.1237	0.1237
72	0.0000	0.1212	0.1212
73	0.0000	0.1233	0.1233
74	0.0000	0.1227	0.1227
75	0.0000	0.1215	0.1215
76	0.0000	0.1235	0.1235
77	0.0000	0.1230	0.1230
78	0.0000	0.1226	0.1226
79	0.0000	0.1196	0.1196
80	0.0000	0.1225	0.1225
81	0.0000	0.1211	0.1211
82	0.0000	0.1216	0.1216
83	0.0000	0.1198	0.1198
84	0.0000	0.1208	0.1208
85	0.0000	0.1219	0.1219
86	0.0000	0.1245	0.1245
87	0.0000	0.1238	0.1238
88	0.0000	0.1223	0.1223
89	0.0000	0.1215	0.1215
90	0.0000	0.1218	0.1218
91	0.0000	0.1220	0.1220
92	0.0000	0.1220	0.1220
93	0.0000	0.1220	0.1220
94	0.0000	0.1244	0.1244
95	0.0000	0.1230	0.1230
96	0.0000	0.1211	0.1211
97	0.0000	0.1208	0.1208
98	0.0000	0.1212	0.1212
99	0.0000	0.1207	0.1207
0.0000 0.1196 0.1196
train_epochs: 100
dropout: 1
save_model: model_saved.npz
filter_shape: [(96, 3, 3), (96, 3, 3), (96, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 15
lr: 1.0
std_p: 0.1
train_on_valid: 1
0	0.7113	0.5493	0.5493
1	0.5121	0.4097	0.4097
2	0.4050	0.3102	0.3102
3	0.3283	0.2952	0.2952
4	0.2772	0.2350	0.2350
5	0.2402	0.2658	0.2658
6	0.2090	0.2333	0.2333
7	0.1810	0.1808	0.1808
8	0.1626	0.2092	0.2092
9	0.1464	0.1728	0.1728
10	0.1294	0.1894	0.1894
11	0.1170	0.1545	0.1545
12	0.1091	0.1743	0.1743
13	0.0958	0.1591	0.1591
14	0.0880	0.1902	0.1902
15	0.0810	0.1514	0.1514
16	0.0718	0.1696	0.1696
17	0.0639	0.1686	0.1686
18	0.0580	0.1508	0.1508
19	0.0520	0.1641	0.1641
20	0.0477	0.1417	0.1417
21	0.0430	0.1499	0.1499
22	0.0398	0.1449	0.1449
23	0.0349	0.1538	0.1538
24	0.0332	0.1459	0.1459
25	0.0290	0.1601	0.1601
26	0.0272	0.1451	0.1451
27	0.0247	0.1426	0.1426
28	0.0221	0.1489	0.1489
29	0.0204	0.1416	0.1416
30	0.0192	0.1417	0.1417
31	0.0179	0.1581	0.1581
32	0.0153	0.1456	0.1456
33	0.0136	0.1413	0.1413
34	0.0132	0.1342	0.1342
35	0.0116	0.1430	0.1430
36	0.0108	0.1433	0.1433
37	0.0097	0.1424	0.1424
38	0.0096	0.1379	0.1379
39	0.0091	0.1420	0.1420
40	0.0082	0.1495	0.1495
41	0.0072	0.1485	0.1485
42	0.0073	0.1399	0.1399
43	0.0066	0.1424	0.1424
44	0.0058	0.1389	0.1389
45	0.0059	0.1428	0.1428
46	0.0049	0.1357	0.1357
47	0.0049	0.1362	0.1362
48	0.0047	0.1406	0.1406
49	0.0042	0.1500	0.1500
50	0.0038	0.1365	0.1365
51	0.0015	0.1291	0.1291
52	0.0013	0.1327	0.1327
53	0.0012	0.1309	0.1309
54	0.0012	0.1269	0.1269
55	0.0010	0.1272	0.1272
56	0.0010	0.1374	0.1374
57	0.0009	0.1280	0.1280
58	0.0008	0.1306	0.1306
59	0.0009	0.1291	0.1291
60	0.0008	0.1318	0.1318
61	0.0008	0.1275	0.1275
62	0.0008	0.1331	0.1331
63	0.0007	0.1300	0.1300
64	0.0008	0.1304	0.1304
65	0.0006	0.1312	0.1312
66	0.0006	0.1315	0.1315
67	0.0007	0.1299	0.1299
68	0.0007	0.1315	0.1315
69	0.0006	0.1313	0.1313
70	0.0006	0.1359	0.1359
71	0.0005	0.1284	0.1284
72	0.0005	0.1261	0.1261
73	0.0005	0.1271	0.1271
74	0.0005	0.1235	0.1235
75	0.0005	0.1272	0.1272
76	0.0004	0.1279	0.1279
77	0.0005	0.1253	0.1253
78	0.0004	0.1288	0.1288
79	0.0004	0.1291	0.1291
80	0.0004	0.1277	0.1277
81	0.0004	0.1263	0.1263
82	0.0004	0.1274	0.1274
83	0.0004	0.1264	0.1264
84	0.0004	0.1280	0.1280
85	0.0004	0.1266	0.1266
86	0.0004	0.1268	0.1268
87	0.0004	0.1306	0.1306
88	0.0004	0.1275	0.1275
89	0.0004	0.1271	0.1271
90	0.0004	0.1289	0.1289
91	0.0004	0.1284	0.1284
92	0.0004	0.1253	0.1253
93	0.0003	0.1258	0.1258
94	0.0003	0.1263	0.1263
95	0.0003	0.1249	0.1249
96	0.0003	0.1256	0.1256
97	0.0003	0.1257	0.1257
98	0.0003	0.1267	0.1267
99	0.0003	0.1264	0.1264
0.0005 0.1235 0.1235
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: none
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
0	0.6947	0.5861	0.5861
1	0.5533	0.4659	0.4659
2	0.4811	0.4475	0.4475
3	0.4429	0.4037	0.4037
4	0.4080	0.4240	0.4240
5	0.3828	0.4138	0.4138
6	0.3648	0.3615	0.3615
7	0.3442	0.3839	0.3839
8	0.3326	0.3253	0.3253
9	0.3225	0.3998	0.3998
10	0.3045	0.3121	0.3121
11	0.2960	0.2793	0.2793
12	0.2883	0.2626	0.2626
13	0.2804	0.2663	0.2663
14	0.2674	0.3335	0.3335
15	0.2643	0.2887	0.2887
16	0.2583	0.2568	0.2568
17	0.2497	0.2876	0.2876
18	0.2489	0.2800	0.2800
19	0.2386	0.2279	0.2279
0.2386 0.2279 0.2279
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
0	0.2399	0.2714	0.2714
1	0.2339	0.2560	0.2560
2	0.2288	0.2747	0.2747
3	0.2225	0.2411	0.2411
4	0.2210	0.2295	0.2295
5	0.2180	0.2595	0.2595
6	0.2132	0.2224	0.2224
7	0.2111	0.2332	0.2332
8	0.2057	0.2287	0.2287
9	0.2068	0.2692	0.2692
10	0.2026	0.2277	0.2277
11	0.1988	0.2305	0.2305
12	0.1973	0.2624	0.2624
13	0.1947	0.2299	0.2299
14	0.1953	0.2135	0.2135
15	0.1913	0.2214	0.2214
16	0.1889	0.2202	0.2202
17	0.1859	0.2032	0.2032
18	0.1851	0.2296	0.2296
19	0.1835	0.2404	0.2404
0.1859 0.2032 0.2032
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 0.1
std_p: 0.5
train_on_valid: 1
0	0.1464	0.1983	0.1983
1	0.1417	0.1970	0.1970
2	0.1392	0.1961	0.1961
3	0.1389	0.1973	0.1973
4	0.1387	0.1934	0.1934
5	0.1374	0.1923	0.1923
6	0.1363	0.1905	0.1905
7	0.1363	0.1892	0.1892
8	0.1343	0.1893	0.1893
9	0.1342	0.1948	0.1948
10	0.1341	0.1977	0.1977
11	0.1340	0.1978	0.1978
12	0.1333	0.1996	0.1996
13	0.1317	0.1949	0.1949
14	0.1324	0.1884	0.1884
15	0.1309	0.1897	0.1897
16	0.1317	0.1946	0.1946
17	0.1306	0.1923	0.1923
18	0.1310	0.1935	0.1935
19	0.1304	0.1920	0.1920
0.1324 0.1884 0.1884
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 0.01
std_p: 0.5
train_on_valid: 1
0	0.1276	0.1922	0.1922
1	0.1275	0.1880	0.1880
2	0.1274	0.1906	0.1906
3	0.1272	0.1921	0.1921
4	0.1271	0.1909	0.1909
5	0.1262	0.1904	0.1904
6	0.1268	0.1893	0.1893
7	0.1264	0.1891	0.1891
8	0.1268	0.1894	0.1894
9	0.1268	0.1883	0.1883
10	0.1271	0.1896	0.1896
11	0.1269	0.1896	0.1896
12	0.1264	0.1909	0.1909
13	0.1266	0.1897	0.1897
14	0.1264	0.1899	0.1899
15	0.1265	0.1894	0.1894
16	0.1257	0.1900	0.1900
17	0.1262	0.1902	0.1902
18	0.1274	0.1925	0.1925
19	0.1260	0.1898	0.1898
0.1275 0.1880 0.1880
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
0	0.6979	0.5256	0.5256
1	0.5010	0.4641	0.4641
2	0.4155	0.3947	0.3947
3	0.3695	0.3481	0.3481
4	0.3317	0.2980	0.2980
5	0.3063	0.3304	0.3304
6	0.2789	0.2552	0.2552
7	0.2563	0.2439	0.2439
8	0.2421	0.2403	0.2403
9	0.2247	0.2532	0.2532
10	0.2141	0.2351	0.2351
11	0.2024	0.2528	0.2528
12	0.1918	0.2141	0.2141
13	0.1853	0.1971	0.1971
14	0.1796	0.1865	0.1865
15	0.1737	0.1898	0.1898
16	0.1657	0.1842	0.1842
17	0.1615	0.2051	0.2051
18	0.1552	0.1915	0.1915
19	0.1512	0.1979	0.1979
20	0.1465	0.1835	0.1835
21	0.1447	0.1687	0.1687
22	0.1396	0.1948	0.1948
23	0.1382	0.1689	0.1689
24	0.1335	0.1827	0.1827
25	0.1304	0.1790	0.1790
26	0.1272	0.1645	0.1645
27	0.1247	0.1616	0.1616
28	0.1234	0.1836	0.1836
29	0.1218	0.1894	0.1894
30	0.1172	0.1592	0.1592
31	0.1165	0.1556	0.1556
32	0.1150	0.1649	0.1649
33	0.1107	0.1549	0.1549
34	0.1084	0.1505	0.1505
35	0.1075	0.1488	0.1488
36	0.1086	0.1582	0.1582
37	0.1046	0.1695	0.1695
38	0.1040	0.1550	0.1550
39	0.1032	0.1430	0.1430
40	0.1012	0.1584	0.1584
41	0.0983	0.1516	0.1516
42	0.0998	0.1559	0.1559
43	0.0962	0.1513	0.1513
44	0.0949	0.1626	0.1626
45	0.0940	0.1471	0.1471
46	0.0913	0.1425	0.1425
47	0.0918	0.1510	0.1510
48	0.0903	0.1423	0.1423
49	0.0883	0.1414	0.1414
50	0.0892	0.1467	0.1467
51	0.0873	0.1402	0.1402
52	0.0860	0.1428	0.1428
53	0.0837	0.1384	0.1384
54	0.0860	0.1389	0.1389
55	0.0848	0.1521	0.1521
56	0.0834	0.1441	0.1441
57	0.0817	0.1377	0.1377
58	0.0818	0.1372	0.1372
59	0.0799	0.1437	0.1437
60	0.0808	0.1502	0.1502
61	0.0803	0.1464	0.1464
62	0.0784	0.1344	0.1344
63	0.0773	0.1374	0.1374
64	0.0786	0.1477	0.1477
65	0.0773	0.1363	0.1363
66	0.0778	0.1512	0.1512
67	0.0770	0.1484	0.1484
68	0.0751	0.1322	0.1322
69	0.0726	0.1425	0.1425
70	0.0740	0.1306	0.1306
71	0.0708	0.1340	0.1340
72	0.0711	0.1458	0.1458
73	0.0708	0.1473	0.1473
74	0.0701	0.1395	0.1395
75	0.0690	0.1446	0.1446
76	0.0698	0.1356	0.1356
77	0.0684	0.1354	0.1354
78	0.0670	0.1335	0.1335
79	0.0666	0.1364	0.1364
80	0.0662	0.1327	0.1327
81	0.0649	0.1298	0.1298
82	0.0653	0.1306	0.1306
83	0.0668	0.1281	0.1281
84	0.0652	0.1375	0.1375
85	0.0639	0.1373	0.1373
86	0.0639	0.1312	0.1312
87	0.0637	0.1387	0.1387
88	0.0635	0.1416	0.1416
89	0.0620	0.1325	0.1325
90	0.0617	0.1364	0.1364
91	0.0617	0.1317	0.1317
92	0.0617	0.1334	0.1334
93	0.0610	0.1287	0.1287
94	0.0610	0.1296	0.1296
95	0.0605	0.1277	0.1277
96	0.0603	0.1309	0.1309
97	0.0596	0.1329	0.1329
98	0.0598	0.1270	0.1270
99	0.0591	0.1282	0.1282
0.0598 0.1270 0.1270
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 0.1
std_p: 0.5
train_on_valid: 1
0	0.0461	0.1235	0.1235
1	0.0428	0.1233	0.1233
2	0.0430	0.1221	0.1221
3	0.0417	0.1227	0.1227
4	0.0415	0.1210	0.1210
5	0.0413	0.1215	0.1215
6	0.0413	0.1228	0.1228
7	0.0402	0.1225	0.1225
8	0.0394	0.1224	0.1224
9	0.0401	0.1217	0.1217
10	0.0392	0.1221	0.1221
11	0.0390	0.1212	0.1212
12	0.0380	0.1231	0.1231
13	0.0386	0.1221	0.1221
14	0.0377	0.1192	0.1192
15	0.0378	0.1211	0.1211
16	0.0380	0.1228	0.1228
17	0.0371	0.1193	0.1193
18	0.0370	0.1217	0.1217
19	0.0369	0.1201	0.1201
0.0377 0.1192 0.1192
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 20
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 10
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 10
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 10
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 10
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 0
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 10
lr: 1.0
std_p: 0.5
train_on_valid: 1
train_epochs: 20
save_model: small.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.0
pool_size: 4
n_nested: 1
train_on_valid: 1
0	0.6394	0.5377	0.5377
1	0.5140	0.4869	0.4869
2	0.4660	0.4379	0.4379
3	0.4330	0.4658	0.4658
4	0.4109	0.3928	0.3928
5	0.3920	0.4227	0.4227
6	0.3806	0.4147	0.4147
7	0.3690	0.3905	0.3905
8	0.3552	0.4360	0.4360
9	0.3504	0.3561	0.3561
10	0.3354	0.3721	0.3721
11	0.3357	0.3126	0.3126
12	0.3251	0.3573	0.3573
13	0.3213	0.3893	0.3893
14	0.3208	0.3435	0.3435
15	0.3132	0.3107	0.3107
16	0.3064	0.3459	0.3459
17	0.2994	0.3123	0.3123
18	0.2989	0.3764	0.3764
19	0.2932	0.3136	0.3136
0.3132 0.3107 0.3107
train_epochs: 0
save_model: small.npz
filter_shape: [(64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 10
lr: 1.0
std_p: 0.0
pool_size: 4
n_nested: 1
train_on_valid: 1
train_epochs: 100
save_model: hashing.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.5
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.6808	0.6689	0.6689
1	0.4943	0.6728	0.6728
2	0.4034	0.4403	0.4403
3	0.3542	0.3712	0.3712
4	0.3157	0.3498	0.3498
5	0.2833	0.3225	0.3225
6	0.2622	0.3040	0.3040
7	0.2445	0.2943	0.2943
8	0.2263	0.2806	0.2806
9	0.2139	0.2646	0.2646
10	0.2059	0.2772	0.2772
11	0.1972	0.2514	0.2514
12	0.1883	0.2576	0.2576
13	0.1829	0.2503	0.2503
14	0.1765	0.2385	0.2385
15	0.1702	0.2377	0.2377
16	0.1651	0.2268	0.2268
17	0.1615	0.2226	0.2226
18	0.1595	0.2141	0.2141
19	0.1520	0.2160	0.2160
20	0.1506	0.2151	0.2151
21	0.1467	0.2219	0.2219
22	0.1448	0.2233	0.2233
23	0.1422	0.2240	0.2240
24	0.1414	0.2233	0.2233
25	0.1382	0.2082	0.2082
26	0.1352	0.1966	0.1966
27	0.1311	0.2091	0.2091
28	0.1303	0.2238	0.2238
29	0.1288	0.2335	0.2335
30	0.1291	0.1955	0.1955
31	0.1268	0.1948	0.1948
32	0.1265	0.2011	0.2011
33	0.1208	0.1949	0.1949
34	0.1231	0.1913	0.1913
35	0.1186	0.1985	0.1985
36	0.1179	0.2136	0.2136
37	0.1151	0.1921	0.1921
38	0.1155	0.1926	0.1926
39	0.1157	0.1967	0.1967
40	0.1139	0.1906	0.1906
41	0.1120	0.1866	0.1866
42	0.1103	0.1837	0.1837
43	0.1102	0.1916	0.1916
44	0.1087	0.1862	0.1862
45	0.1102	0.1802	0.1802
46	0.1081	0.1844	0.1844
47	0.1051	0.1773	0.1773
48	0.1081	0.1782	0.1782
49	0.1075	0.1860	0.1860
50	0.1039	0.1824	0.1824
51	0.1046	0.1885	0.1885
52	0.1036	0.1750	0.1750
53	0.1020	0.1782	0.1782
54	0.1005	0.1734	0.1734
55	0.0987	0.1761	0.1761
56	0.0994	0.1718	0.1718
57	0.0989	0.1790	0.1790
58	0.0975	0.1708	0.1708
59	0.0966	0.1705	0.1705
60	0.0971	0.1748	0.1748
61	0.0947	0.1980	0.1980
62	0.0958	0.1767	0.1767
63	0.0951	0.1780	0.1780
64	0.0949	0.1794	0.1794
65	0.0922	0.1831	0.1831
66	0.0922	0.1754	0.1754
67	0.0926	0.1622	0.1622
68	0.0923	0.1714	0.1714
69	0.0914	0.1725	0.1725
70	0.0902	0.1727	0.1727
71	0.0902	0.1610	0.1610
72	0.0886	0.1708	0.1708
73	0.0891	0.1616	0.1616
74	0.0886	0.1626	0.1626
75	0.0873	0.1751	0.1751
76	0.0863	0.1697	0.1697
77	0.0880	0.1658	0.1658
78	0.0861	0.1699	0.1699
79	0.0862	0.1635	0.1635
80	0.0843	0.1666	0.1666
81	0.0859	0.1662	0.1662
82	0.0836	0.1645	0.1645
83	0.0823	0.1604	0.1604
84	0.0798	0.1639	0.1639
85	0.0786	0.1598	0.1598
86	0.0787	0.1534	0.1534
87	0.0782	0.1579	0.1579
88	0.0774	0.1502	0.1502
89	0.0767	0.1588	0.1588
90	0.0771	0.1579	0.1579
91	0.0767	0.1558	0.1558
92	0.0745	0.1601	0.1601
93	0.0746	0.1570	0.1570
94	0.0751	0.1659	0.1659
95	0.0737	0.1632	0.1632
96	0.0737	0.1551	0.1551
97	0.0732	0.1607	0.1607
98	0.0736	0.1593	0.1593
99	0.0738	0.1506	0.1506
0.0774 0.1502 0.1502
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.5
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7508	0.5926	0.5926
1	0.5240	0.4357	0.4357
2	0.4166	0.3608	0.3608
3	0.3718	0.3212	0.3212
4	0.3324	0.3840	0.3840
5	0.3012	0.3665	0.3665
6	0.2749	0.2510	0.2510
7	0.2543	0.2815	0.2815
8	0.2383	0.2753	0.2753
9	0.2272	0.2505	0.2505
10	0.2155	0.2566	0.2566
11	0.2078	0.2352	0.2352
12	0.1955	0.2136	0.2136
13	0.1908	0.2313	0.2313
14	0.1844	0.3067	0.3067
15	0.1764	0.2066	0.2066
16	0.1698	0.2083	0.2083
17	0.1676	0.2110	0.2110
18	0.1644	0.2035	0.2035
19	0.1570	0.2063	0.2063
20	0.1541	0.2217	0.2217
21	0.1507	0.1948	0.1948
22	0.1455	0.2321	0.2321
23	0.1420	0.2059	0.2059
24	0.1377	0.1906	0.1906
25	0.1360	0.2282	0.2282
26	0.1347	0.2120	0.2120
27	0.1317	0.1946	0.1946
28	0.1297	0.2112	0.2112
29	0.1270	0.1839	0.1839
30	0.1249	0.1827	0.1827
31	0.1237	0.1697	0.1697
32	0.1207	0.1772	0.1772
33	0.1191	0.1991	0.1991
34	0.1155	0.1859	0.1859
35	0.1177	0.2038	0.2038
36	0.1148	0.1783	0.1783
37	0.1123	0.1915	0.1915
38	0.1117	0.2182	0.2182
39	0.1082	0.1654	0.1654
40	0.1066	0.1906	0.1906
41	0.1066	0.1763	0.1763
42	0.1033	0.1822	0.1822
43	0.1049	0.1755	0.1755
44	0.1027	0.1813	0.1813
45	0.0989	0.1845	0.1845
46	0.0979	0.1814	0.1814
47	0.1017	0.1739	0.1739
48	0.0991	0.1805	0.1805
49	0.0987	0.1713	0.1713
50	0.0943	0.1946	0.1946
51	0.0794	0.1617	0.1617
52	0.0752	0.1686	0.1686
53	0.0718	0.1687	0.1687
54	0.0700	0.1727	0.1727
55	0.0693	0.1601	0.1601
56	0.0702	0.1731	0.1731
57	0.0697	0.1634	0.1634
58	0.0688	0.1798	0.1798
59	0.0689	0.1650	0.1650
60	0.0668	0.1695	0.1695
61	0.0652	0.1611	0.1611
62	0.0643	0.1824	0.1824
63	0.0623	0.1797	0.1797
64	0.0636	0.1666	0.1666
65	0.0629	0.1646	0.1646
66	0.0628	0.1583	0.1583
67	0.0610	0.1713	0.1713
68	0.0633	0.1727	0.1727
69	0.0602	0.1634	0.1634
70	0.0605	0.1567	0.1567
71	0.0598	0.1592	0.1592
72	0.0583	0.1669	0.1669
73	0.0604	0.1719	0.1719
74	0.0569	0.1768	0.1768
75	0.0579	0.1671	0.1671
76	0.0563	0.1638	0.1638
77	0.0578	0.1660	0.1660
78	0.0565	0.1567	0.1567
79	0.0542	0.1755	0.1755
80	0.0557	0.1654	0.1654
81	0.0554	0.1746	0.1746
82	0.0550	0.1571	0.1571
83	0.0536	0.1664	0.1664
84	0.0569	0.1707	0.1707
85	0.0548	0.1592	0.1592
86	0.0536	0.1800	0.1800
87	0.0537	0.1632	0.1632
88	0.0533	0.1702	0.1702
89	0.0523	0.1661	0.1661
90	0.0441	0.1676	0.1676
91	0.0435	0.1612	0.1612
92	0.0426	0.1629	0.1629
93	0.0408	0.1598	0.1598
94	0.0413	0.1644	0.1644
95	0.0416	0.1589	0.1589
96	0.0411	0.1616	0.1616
97	0.0396	0.1620	0.1620
98	0.0374	0.1645	0.1645
99	0.0402	0.1635	0.1635
0.0565 0.1567 0.1567
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.5
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7809	0.7288	0.7288
1	0.5412	0.5114	0.5114
2	0.4340	0.3579	0.3579
3	0.3848	0.3569	0.3569
4	0.3492	0.3401	0.3401
5	0.3129	0.2861	0.2861
6	0.2862	0.3839	0.3839
7	0.2657	0.2504	0.2504
8	0.2509	0.2582	0.2582
9	0.2367	0.2594	0.2594
10	0.2221	0.2459	0.2459
11	0.2179	0.2519	0.2519
12	0.2047	0.2367	0.2367
13	0.2003	0.2531	0.2531
14	0.1927	0.2168	0.2168
15	0.1833	0.2107	0.2107
16	0.1812	0.2129	0.2129
17	0.1745	0.2127	0.2127
18	0.1695	0.2021	0.2021
19	0.1636	0.2194	0.2194
20	0.1598	0.2029	0.2029
21	0.1566	0.1934	0.1934
22	0.1575	0.2052	0.2052
23	0.1530	0.2031	0.2031
24	0.1448	0.1943	0.1943
25	0.1422	0.2025	0.2025
26	0.1403	0.2042	0.2042
27	0.1370	0.1926	0.1926
28	0.1359	0.1910	0.1910
29	0.1319	0.1990	0.1990
30	0.1299	0.1978	0.1978
31	0.1271	0.1738	0.1738
32	0.1236	0.1923	0.1923
33	0.1228	0.1872	0.1872
34	0.1219	0.1955	0.1955
35	0.1224	0.1787	0.1787
36	0.1194	0.2096	0.2096
37	0.1138	0.1765	0.1765
38	0.1145	0.1748	0.1748
39	0.1112	0.1708	0.1708
40	0.1104	0.1800	0.1800
41	0.1106	0.1853	0.1853
42	0.1101	0.1842	0.1842
43	0.1070	0.1746	0.1746
44	0.1060	0.1835	0.1835
45	0.1046	0.1743	0.1743
46	0.1028	0.1880	0.1880
47	0.1008	0.1743	0.1743
48	0.1007	0.1664	0.1664
49	0.0979	0.1645	0.1645
50	0.0958	0.1767	0.1767
51	0.0959	0.1771	0.1771
52	0.0967	0.1857	0.1857
53	0.0936	0.1787	0.1787
54	0.0946	0.1935	0.1935
55	0.0946	0.1727	0.1727
56	0.0911	0.1739	0.1739
57	0.0888	0.1692	0.1692
58	0.0895	0.1779	0.1779
59	0.0882	0.1768	0.1768
60	0.0876	0.1659	0.1659
61	0.0696	0.1626	0.1626
62	0.0688	0.1526	0.1526
63	0.0663	0.1540	0.1540
64	0.0660	0.1602	0.1602
65	0.0650	0.1607	0.1607
66	0.0632	0.1652	0.1652
67	0.0633	0.1626	0.1626
68	0.0630	0.1620	0.1620
69	0.0635	0.1645	0.1645
70	0.0630	0.1652	0.1652
71	0.0621	0.1639	0.1639
72	0.0622	0.1636	0.1636
73	0.0614	0.1614	0.1614
74	0.0549	0.1609	0.1609
75	0.0533	0.1610	0.1610
76	0.0520	0.1608	0.1608
77	0.0521	0.1658	0.1658
78	0.0511	0.1620	0.1620
79	0.0507	0.1618	0.1618
80	0.0511	0.1629	0.1629
81	0.0507	0.1580	0.1580
82	0.0493	0.1611	0.1611
83	0.0491	0.1582	0.1582
84	0.0492	0.1649	0.1649
85	0.0468	0.1607	0.1607
86	0.0452	0.1594	0.1594
87	0.0445	0.1569	0.1569
88	0.0438	0.1561	0.1561
89	0.0437	0.1575	0.1575
90	0.0434	0.1584	0.1584
91	0.0442	0.1547	0.1547
92	0.0428	0.1590	0.1590
93	0.0437	0.1593	0.1593
94	0.0432	0.1565	0.1565
95	0.0419	0.1576	0.1576
96	0.0417	0.1576	0.1576
97	0.0412	0.1561	0.1561
98	0.0410	0.1558	0.1558
99	0.0410	0.1542	0.1542
0.0688 0.1526 0.1526
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.5
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7302	0.5354	0.5354
1	0.5099	0.4259	0.4259
2	0.4090	0.3625	0.3625
3	0.3553	0.4329	0.4329
4	0.3166	0.3137	0.3137
5	0.2876	0.2812	0.2812
6	0.2625	0.2517	0.2517
7	0.2471	0.2526	0.2526
8	0.2314	0.2466	0.2466
9	0.2205	0.2404	0.2404
10	0.2080	0.2602	0.2602
11	0.2019	0.2418	0.2418
12	0.1902	0.2275	0.2275
13	0.1794	0.2242	0.2242
14	0.1778	0.2324	0.2324
15	0.1659	0.2189	0.2189
16	0.1653	0.2167	0.2167
17	0.1619	0.2325	0.2325
18	0.1535	0.2015	0.2015
19	0.1511	0.2041	0.2041
20	0.1469	0.2014	0.2014
21	0.1445	0.1956	0.1956
22	0.1356	0.1968	0.1968
23	0.1353	0.1973	0.1973
24	0.1329	0.1986	0.1986
25	0.1299	0.1881	0.1881
26	0.1234	0.2024	0.2024
27	0.1239	0.1964	0.1964
28	0.1197	0.1807	0.1807
29	0.1156	0.1894	0.1894
30	0.1158	0.1872	0.1872
31	0.1116	0.1895	0.1895
32	0.1103	0.2071	0.2071
33	0.1074	0.1939	0.1939
34	0.1041	0.1911	0.1911
35	0.1036	0.1861	0.1861
36	0.0995	0.2169	0.2169
37	0.1031	0.2009	0.2009
38	0.0989	0.1890	0.1890
39	0.0961	0.1992	0.1992
40	0.0771	0.1796	0.1796
41	0.0728	0.1742	0.1742
42	0.0714	0.1720	0.1720
43	0.0675	0.1745	0.1745
44	0.0659	0.1757	0.1757
45	0.0665	0.1720	0.1720
46	0.0636	0.1723	0.1723
47	0.0626	0.1738	0.1738
48	0.0601	0.1731	0.1731
49	0.0622	0.1717	0.1717
50	0.0610	0.1701	0.1701
51	0.0592	0.1735	0.1735
52	0.0540	0.1839	0.1839
53	0.0586	0.1815	0.1815
54	0.0586	0.1699	0.1699
55	0.0558	0.1709	0.1709
56	0.0552	0.1737	0.1737
57	0.0534	0.1639	0.1639
58	0.0515	0.1787	0.1787
59	0.0531	0.1815	0.1815
60	0.0550	0.1720	0.1720
61	0.0515	0.1725	0.1725
62	0.0508	0.1704	0.1704
63	0.0506	0.1786	0.1786
64	0.0495	0.1744	0.1744
65	0.0494	0.1685	0.1685
66	0.0484	0.1737	0.1737
67	0.0469	0.1697	0.1697
68	0.0477	0.1767	0.1767
69	0.0417	0.1701	0.1701
70	0.0390	0.1723	0.1723
71	0.0383	0.1777	0.1777
72	0.0369	0.1691	0.1691
73	0.0358	0.1655	0.1655
74	0.0357	0.1719	0.1719
75	0.0351	0.1681	0.1681
76	0.0338	0.1717	0.1717
77	0.0351	0.1721	0.1721
78	0.0346	0.1693	0.1693
79	0.0329	0.1686	0.1686
80	0.0282	0.1681	0.1681
81	0.0280	0.1678	0.1678
82	0.0276	0.1680	0.1680
83	0.0282	0.1689	0.1689
84	0.0267	0.1683	0.1683
85	0.0253	0.1679	0.1679
86	0.0258	0.1688	0.1688
87	0.0249	0.1671	0.1671
88	0.0251	0.1665	0.1665
89	0.0257	0.1682	0.1682
90	0.0247	0.1674	0.1674
91	0.0229	0.1669	0.1669
92	0.0225	0.1686	0.1686
93	0.0224	0.1667	0.1667
94	0.0226	0.1670	0.1670
95	0.0219	0.1663	0.1663
96	0.0218	0.1650	0.1650
97	0.0218	0.1669	0.1669
98	0.0225	0.1663	0.1663
99	0.0213	0.1664	0.1664
0.0534 0.1639 0.1639
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.5
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.6975	0.5561	0.5561
1	0.4932	0.4672	0.4672
2	0.4022	0.3656	0.3656
3	0.3451	0.3273	0.3273
4	0.3127	0.2866	0.2866
5	0.2837	0.2835	0.2835
6	0.2619	0.2681	0.2681
7	0.2440	0.2402	0.2402
8	0.2290	0.2485	0.2485
9	0.2124	0.2278	0.2278
10	0.2051	0.2199	0.2199
11	0.1911	0.2126	0.2126
12	0.1877	0.2060	0.2060
13	0.1776	0.1879	0.1879
14	0.1711	0.2060	0.2060
15	0.1633	0.1911	0.1911
16	0.1620	0.1934	0.1934
17	0.1544	0.1789	0.1789
18	0.1505	0.1741	0.1741
19	0.1471	0.1735	0.1735
20	0.1434	0.1812	0.1812
21	0.1375	0.1686	0.1686
22	0.1359	0.1918	0.1918
23	0.1316	0.1739	0.1739
24	0.1289	0.1788	0.1788
25	0.1281	0.1849	0.1849
26	0.1233	0.1631	0.1631
27	0.1199	0.1651	0.1651
28	0.1194	0.1639	0.1639
29	0.1196	0.1684	0.1684
30	0.1172	0.1641	0.1641
31	0.1130	0.1728	0.1728
32	0.1120	0.1660	0.1660
33	0.1111	0.1643	0.1643
34	0.1066	0.1624	0.1624
35	0.1032	0.1707	0.1707
36	0.1047	0.1681	0.1681
37	0.1026	0.1640	0.1640
38	0.1004	0.1566	0.1566
39	0.0982	0.1576	0.1576
40	0.1003	0.1616	0.1616
41	0.0954	0.1526	0.1526
42	0.0937	0.1561	0.1561
43	0.0916	0.1568	0.1568
44	0.0909	0.1586	0.1586
45	0.0918	0.1631	0.1631
46	0.0898	0.1563	0.1563
47	0.0892	0.1620	0.1620
48	0.0876	0.1491	0.1491
49	0.0861	0.1497	0.1497
50	0.0861	0.1495	0.1495
51	0.0825	0.1527	0.1527
52	0.0824	0.1455	0.1455
53	0.0792	0.1470	0.1470
54	0.0818	0.1581	0.1581
55	0.0812	0.1492	0.1492
56	0.0784	0.1587	0.1587
57	0.0783	0.1540	0.1540
58	0.0769	0.1428	0.1428
59	0.0777	0.1378	0.1378
60	0.0792	0.1557	0.1557
61	0.0758	0.1518	0.1518
62	0.0731	0.1502	0.1502
63	0.0738	0.1486	0.1486
64	0.0735	0.1471	0.1471
65	0.0727	0.1499	0.1499
66	0.0725	0.1480	0.1480
67	0.0705	0.1464	0.1464
68	0.0702	0.1447	0.1447
69	0.0704	0.1453	0.1453
70	0.0704	0.1401	0.1401
71	0.0567	0.1310	0.1310
72	0.0547	0.1319	0.1319
73	0.0531	0.1330	0.1330
74	0.0530	0.1399	0.1399
75	0.0520	0.1431	0.1431
76	0.0515	0.1409	0.1409
77	0.0527	0.1350	0.1350
78	0.0496	0.1412	0.1412
79	0.0488	0.1386	0.1386
80	0.0479	0.1329	0.1329
81	0.0482	0.1416	0.1416
82	0.0477	0.1330	0.1330
83	0.0423	0.1335	0.1335
84	0.0410	0.1360	0.1360
85	0.0405	0.1319	0.1319
86	0.0396	0.1365	0.1365
87	0.0401	0.1327	0.1327
88	0.0404	0.1332	0.1332
89	0.0385	0.1307	0.1307
90	0.0379	0.1330	0.1330
91	0.0384	0.1316	0.1316
92	0.0386	0.1310	0.1310
93	0.0386	0.1322	0.1322
94	0.0390	0.1298	0.1298
95	0.0375	0.1299	0.1299
96	0.0372	0.1296	0.1296
97	0.0372	0.1325	0.1325
98	0.0367	0.1292	0.1292
99	0.0371	0.1299	0.1299
0.0367 0.1292 0.1292
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.5
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7423	0.5796	0.5796
1	0.5194	0.4382	0.4382
2	0.4136	0.3626	0.3626
3	0.3561	0.3739	0.3739
4	0.3144	0.3068	0.3068
5	0.2849	0.2961	0.2961
6	0.2599	0.2828	0.2828
7	0.2434	0.2397	0.2397
8	0.2316	0.2205	0.2205
9	0.2147	0.2545	0.2545
10	0.2039	0.2145	0.2145
11	0.1921	0.2229	0.2229
12	0.1801	0.2282	0.2282
13	0.1751	0.2270	0.2270
14	0.1690	0.2081	0.2081
15	0.1619	0.2041	0.2041
16	0.1574	0.1906	0.1906
17	0.1477	0.2234	0.2234
18	0.1466	0.1825	0.1825
19	0.1394	0.2069	0.2069
20	0.1343	0.1868	0.1868
21	0.1311	0.1880	0.1880
22	0.1308	0.2074	0.2074
23	0.1257	0.2012	0.2012
24	0.1241	0.1882	0.1882
25	0.1172	0.1816	0.1816
26	0.1172	0.2156	0.2156
27	0.1109	0.1785	0.1785
28	0.1110	0.1918	0.1918
29	0.1069	0.1877	0.1877
30	0.1055	0.1907	0.1907
31	0.1024	0.1918	0.1918
32	0.1019	0.1967	0.1967
33	0.1004	0.1849	0.1849
34	0.0958	0.1861	0.1861
35	0.0930	0.1739	0.1739
36	0.0923	0.1787	0.1787
37	0.0887	0.1848	0.1848
38	0.0896	0.1737	0.1737
39	0.0884	0.2069	0.2069
40	0.0844	0.1879	0.1879
41	0.0841	0.1693	0.1693
42	0.0838	0.1789	0.1789
43	0.0800	0.1836	0.1836
44	0.0792	0.1808	0.1808
45	0.0783	0.1821	0.1821
46	0.0756	0.1776	0.1776
47	0.0740	0.1730	0.1730
48	0.0737	0.1807	0.1807
49	0.0725	0.1664	0.1664
50	0.0706	0.1792	0.1792
51	0.0723	0.1753	0.1753
52	0.0692	0.1847	0.1847
53	0.0658	0.1792	0.1792
54	0.0657	0.1756	0.1756
55	0.0665	0.1722	0.1722
56	0.0638	0.1836	0.1836
57	0.0650	0.1903	0.1903
58	0.0628	0.1826	0.1826
59	0.0614	0.1696	0.1696
60	0.0624	0.1752	0.1752
61	0.0439	0.1588	0.1588
62	0.0429	0.1620	0.1620
63	0.0392	0.1588	0.1588
64	0.0379	0.1603	0.1603
65	0.0372	0.1598	0.1598
66	0.0369	0.1684	0.1684
67	0.0359	0.1720	0.1720
68	0.0368	0.1624	0.1624
69	0.0340	0.1626	0.1626
70	0.0343	0.1635	0.1635
71	0.0346	0.1692	0.1692
72	0.0329	0.1673	0.1673
73	0.0340	0.1612	0.1612
74	0.0320	0.1621	0.1621
75	0.0269	0.1590	0.1590
76	0.0261	0.1637	0.1637
77	0.0246	0.1588	0.1588
78	0.0232	0.1609	0.1609
79	0.0237	0.1592	0.1592
80	0.0227	0.1597	0.1597
81	0.0230	0.1612	0.1612
82	0.0216	0.1629	0.1629
83	0.0220	0.1632	0.1632
84	0.0215	0.1627	0.1627
85	0.0214	0.1612	0.1612
86	0.0205	0.1654	0.1654
87	0.0195	0.1651	0.1651
88	0.0204	0.1628	0.1628
89	0.0171	0.1620	0.1620
90	0.0173	0.1609	0.1609
91	0.0176	0.1612	0.1612
92	0.0167	0.1646	0.1646
93	0.0166	0.1615	0.1615
94	0.0162	0.1596	0.1596
95	0.0156	0.1606	0.1606
96	0.0161	0.1605	0.1605
97	0.0160	0.1608	0.1608
98	0.0154	0.1631	0.1631
99	0.0159	0.1610	0.1610
0.0246 0.1588 0.1588
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.5
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7340	0.5359	0.5359
1	0.5281	0.6035	0.6035
2	0.4309	0.4026	0.4026
3	0.3722	0.3415	0.3415
4	0.3307	0.3397	0.3397
5	0.2997	0.3057	0.3057
6	0.2770	0.2894	0.2894
7	0.2599	0.2524	0.2524
8	0.2405	0.2803	0.2803
9	0.2291	0.2883	0.2883
10	0.2128	0.2353	0.2353
11	0.2005	0.2424	0.2424
12	0.1971	0.2786	0.2786
13	0.1873	0.2332	0.2332
14	0.1804	0.2476	0.2476
15	0.1624	0.2188	0.2188
16	0.1602	0.2362	0.2362
17	0.1551	0.2186	0.2186
18	0.1468	0.2415	0.2415
19	0.1438	0.2370	0.2370
20	0.1414	0.2403	0.2403
21	0.1359	0.2241	0.2241
22	0.1319	0.2276	0.2276
23	0.1297	0.2307	0.2307
24	0.1224	0.2272	0.2272
25	0.1193	0.2213	0.2213
26	0.1205	0.2264	0.2264
27	0.1135	0.2132	0.2132
28	0.1116	0.2185	0.2185
29	0.1069	0.2029	0.2029
30	0.1011	0.2244	0.2244
31	0.1031	0.2322	0.2322
32	0.0989	0.2335	0.2335
33	0.0940	0.2421	0.2421
34	0.0960	0.2122	0.2122
35	0.0901	0.2214	0.2214
36	0.0905	0.2145	0.2145
37	0.0909	0.2361	0.2361
38	0.0880	0.2092	0.2092
39	0.0838	0.2130	0.2130
40	0.0826	0.2624	0.2624
41	0.0553	0.2062	0.2062
42	0.0507	0.2060	0.2060
43	0.0489	0.2111	0.2111
44	0.0442	0.2067	0.2067
45	0.0456	0.1989	0.1989
46	0.0447	0.2338	0.2338
47	0.0444	0.2124	0.2124
48	0.0418	0.2149	0.2149
49	0.0405	0.2023	0.2023
50	0.0374	0.2019	0.2019
51	0.0382	0.2017	0.2017
52	0.0367	0.2081	0.2081
53	0.0395	0.1966	0.1966
54	0.0373	0.2164	0.2164
55	0.0382	0.2003	0.2003
56	0.0357	0.2209	0.2209
57	0.0345	0.2117	0.2117
58	0.0323	0.2016	0.2016
59	0.0350	0.2135	0.2135
60	0.0328	0.2075	0.2075
61	0.0305	0.2036	0.2036
62	0.0329	0.2404	0.2404
63	0.0330	0.2040	0.2040
64	0.0311	0.2141	0.2141
65	0.0215	0.2226	0.2226
66	0.0197	0.2017	0.2017
67	0.0181	0.2069	0.2069
68	0.0166	0.2028	0.2028
69	0.0168	0.2063	0.2063
70	0.0158	0.2009	0.2009
71	0.0173	0.2049	0.2049
72	0.0169	0.2039	0.2039
73	0.0154	0.2023	0.2023
74	0.0165	0.2034	0.2034
75	0.0141	0.2044	0.2044
76	0.0117	0.2022	0.2022
77	0.0116	0.2046	0.2046
78	0.0111	0.2049	0.2049
79	0.0108	0.2066	0.2066
80	0.0111	0.2066	0.2066
81	0.0103	0.2057	0.2057
82	0.0099	0.2044	0.2044
83	0.0101	0.2060	0.2060
84	0.0104	0.2104	0.2104
85	0.0103	0.2100	0.2100
86	0.0095	0.2041	0.2041
87	0.0083	0.2064	0.2064
88	0.0085	0.2063	0.2063
89	0.0079	0.2050	0.2050
90	0.0082	0.2067	0.2067
91	0.0077	0.2058	0.2058
92	0.0077	0.2058	0.2058
93	0.0074	0.2072	0.2072
94	0.0082	0.2067	0.2067
95	0.0079	0.2058	0.2058
96	0.0078	0.2041	0.2041
97	0.0072	0.2051	0.2051
98	0.0071	0.2091	0.2091
99	0.0068	0.2080	0.2080
0.0395 0.1966 0.1966
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(64, 5, 5), (64, 5, 5), (64, 5, 5)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.0
doubleconv: 0
pool_size: 2
n_nested: 1
train_on_valid: 1
0	0.6488	0.5500	0.5500
1	0.5050	0.4734	0.4734
2	0.4323	0.4557	0.4557
3	0.3889	0.3581	0.3581
4	0.3464	0.2974	0.2974
5	0.3134	0.3108	0.3108
6	0.2928	0.3618	0.3618
7	0.2654	0.3359	0.3359
8	0.2504	0.2806	0.2806
9	0.2325	0.3513	0.3513
10	0.2196	0.2578	0.2578
11	0.2103	0.2783	0.2783
12	0.1903	0.2642	0.2642
13	0.1821	0.2561	0.2561
14	0.1756	0.2640	0.2640
15	0.1614	0.2449	0.2449
16	0.1499	0.2562	0.2562
17	0.1401	0.2354	0.2354
18	0.1288	0.2534	0.2534
19	0.1229	0.2147	0.2147
0.1229 0.2147 0.2147
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(64, 5, 5), (64, 5, 5), (64, 5, 5)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.0
doubleconv: 1
pool_size: 2
n_nested: 1
train_on_valid: 1
0	0.6382	0.4987	0.4987
1	0.4504	0.3741	0.3741
2	0.3665	0.3555	0.3555
3	0.3138	0.2843	0.2843
4	0.2803	0.2793	0.2793
5	0.2506	0.2776	0.2776
6	0.2301	0.2659	0.2659
7	0.2139	0.2522	0.2522
8	0.1945	0.2273	0.2273
9	0.1830	0.2410	0.2410
10	0.1718	0.2356	0.2356
11	0.1562	0.2528	0.2528
12	0.1448	0.2342	0.2342
13	0.1343	0.2280	0.2280
14	0.1267	0.2361	0.2361
15	0.1182	0.2387	0.2387
16	0.1086	0.2272	0.2272
17	0.1005	0.2649	0.2649
18	0.0954	0.2154	0.2154
19	0.0849	0.2421	0.2421
0.0954 0.2154 0.2154
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(64, 7, 7), (64, 7, 7)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 1
train_on_valid: 1
0	0.6711	0.5641	0.5641
1	0.5356	0.5544	0.5544
2	0.4810	0.4414	0.4414
3	0.4480	0.4263	0.4263
4	0.4245	0.3790	0.3790
5	0.4091	0.4323	0.4323
6	0.3873	0.3687	0.3687
7	0.3759	0.4328	0.4328
8	0.3635	0.3772	0.3772
9	0.3526	0.3696	0.3696
10	0.3415	0.3414	0.3414
11	0.3396	0.3439	0.3439
12	0.3281	0.3482	0.3482
13	0.3216	0.3293	0.3293
14	0.3169	0.3089	0.3089
15	0.3047	0.3163	0.3163
16	0.3035	0.3072	0.3072
17	0.2952	0.3651	0.3651
18	0.2901	0.3452	0.3452
19	0.2891	0.3420	0.3420
0.3035 0.3072 0.3072
train_epochs: 5
save_model: model_saved.npz
filter_shape: [(64, 5, 5), (64, 5, 5), (64, 5, 5)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.8189	0.7983	0.7983
1	0.7685	0.7433	0.7433
2	0.7215	0.6826	0.6826
3	0.7123	0.6619	0.6619
4	0.6906	0.7399	0.7399
0.7123 0.6619 0.6619
train_epochs: 5
save_model: model_saved.npz
filter_shape: [(16, 5, 5), (16, 5, 5), (16, 5, 5)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7202	0.5837	0.5837
1	0.5168	0.4040	0.4040
2	0.4224	0.3420	0.3420
3	0.3634	0.3361	0.3361
4	0.3324	0.2961	0.2961
0.3324 0.2961 0.2961
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(64, 5, 5), (64, 5, 5), (64, 5, 5)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.5
doubleconv: 0
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.6936	0.6701	0.6701
1	0.4925	0.3971	0.3971
2	0.3885	0.3788	0.3788
3	0.3221	0.2865	0.2865
4	0.2801	0.3001	0.3001
5	0.2443	0.2701	0.2701
6	0.2230	0.2205	0.2205
7	0.2037	0.2153	0.2153
8	0.1868	0.2348	0.2348
9	0.1743	0.2162	0.2162
10	0.1647	0.1803	0.1803
11	0.1514	0.1900	0.1900
12	0.1455	0.1744	0.1744
13	0.1354	0.1666	0.1666
14	0.1288	0.1732	0.1732
15	0.1220	0.1775	0.1775
16	0.1159	0.1787	0.1787
17	0.1114	0.1699	0.1699
18	0.1090	0.1584	0.1584
19	0.1016	0.1508	0.1508
20	0.0979	0.1682	0.1682
21	0.0908	0.1568	0.1568
22	0.0893	0.1808	0.1808
23	0.0860	0.1503	0.1503
24	0.0813	0.1576	0.1576
25	0.0787	0.1672	0.1672
26	0.0762	0.1566	0.1566
27	0.0728	0.1499	0.1499
28	0.0684	0.1476	0.1476
29	0.0670	0.1472	0.1472
30	0.0655	0.1442	0.1442
31	0.0623	0.1428	0.1428
32	0.0631	0.1484	0.1484
33	0.0616	0.1489	0.1489
34	0.0570	0.1528	0.1528
35	0.0539	0.1431	0.1431
36	0.0510	0.1361	0.1361
37	0.0505	0.1435	0.1435
38	0.0491	0.1467	0.1467
39	0.0483	0.1492	0.1492
40	0.0452	0.1454	0.1454
41	0.0446	0.1399	0.1399
42	0.0438	0.1491	0.1491
43	0.0426	0.1440	0.1440
44	0.0413	0.1413	0.1413
45	0.0396	0.1427	0.1427
46	0.0378	0.1387	0.1387
47	0.0371	0.1460	0.1460
48	0.0235	0.1309	0.1309
49	0.0198	0.1288	0.1288
50	0.0183	0.1276	0.1276
51	0.0180	0.1410	0.1410
52	0.0174	0.1316	0.1316
53	0.0158	0.1274	0.1274
54	0.0155	0.1304	0.1304
55	0.0144	0.1291	0.1291
56	0.0146	0.1261	0.1261
57	0.0141	0.1326	0.1326
58	0.0134	0.1317	0.1317
59	0.0128	0.1304	0.1304
60	0.0124	0.1289	0.1289
61	0.0119	0.1381	0.1381
62	0.0125	0.1300	0.1300
63	0.0116	0.1341	0.1341
64	0.0112	0.1345	0.1345
65	0.0103	0.1315	0.1315
66	0.0110	0.1283	0.1283
67	0.0098	0.1310	0.1310
68	0.0063	0.1280	0.1280
69	0.0053	0.1278	0.1278
70	0.0056	0.1295	0.1295
71	0.0052	0.1244	0.1244
72	0.0049	0.1281	0.1281
73	0.0049	0.1283	0.1283
74	0.0044	0.1266	0.1266
75	0.0040	0.1258	0.1258
76	0.0039	0.1255	0.1255
77	0.0039	0.1258	0.1258
78	0.0039	0.1255	0.1255
79	0.0035	0.1267	0.1267
80	0.0035	0.1265	0.1265
81	0.0038	0.1243	0.1243
82	0.0030	0.1268	0.1268
83	0.0032	0.1288	0.1288
84	0.0032	0.1269	0.1269
85	0.0030	0.1251	0.1251
86	0.0030	0.1243	0.1243
87	0.0030	0.1281	0.1281
88	0.0031	0.1258	0.1258
89	0.0030	0.1264	0.1264
90	0.0030	0.1260	0.1260
91	0.0030	0.1279	0.1279
92	0.0028	0.1269	0.1269
93	0.0025	0.1256	0.1256
94	0.0026	0.1251	0.1251
95	0.0025	0.1254	0.1254
96	0.0026	0.1262	0.1262
97	0.0028	0.1255	0.1255
98	0.0014	0.1243	0.1243
99	0.0015	0.1247	0.1247
0.0014 0.1243 0.1243
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(16, 5, 5), (16, 5, 5), (16, 5, 5)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.3008	0.2734	0.2734
1	0.2751	0.2795	0.2795
2	0.2512	0.3609	0.3609
3	0.2377	0.2272	0.2272
4	0.2176	0.2311	0.2311
5	0.2063	0.2065	0.2065
6	0.1961	0.1930	0.1930
7	0.1854	0.1875	0.1875
8	0.1747	0.2166	0.2166
9	0.1707	0.2248	0.2248
10	0.1624	0.1726	0.1726
11	0.1564	0.1777	0.1777
12	0.1516	0.1795	0.1795
13	0.1500	0.1829	0.1829
14	0.1405	0.1687	0.1687
15	0.1356	0.1793	0.1793
16	0.1300	0.1721	0.1721
17	0.1291	0.1632	0.1632
18	0.1248	0.1704	0.1704
19	0.1176	0.1713	0.1713
0.1291 0.1632 0.1632
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(16, 5, 5), (16, 5, 5), (16, 5, 5)]
batch_size: 200
dataset: cifar10
load_model: 1
patience: 5
lr: 0.5
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.0943	0.1584	0.1584
1	0.0902	0.1468	0.1468
2	0.0875	0.1386	0.1386
3	0.0853	0.1475	0.1475
4	0.0834	0.1454	0.1454
5	0.0819	0.1403	0.1403
6	0.0815	0.1397	0.1397
7	0.0796	0.1414	0.1414
8	0.0773	0.1515	0.1515
9	0.0671	0.1375	0.1375
10	0.0639	0.1365	0.1365
11	0.0637	0.1410	0.1410
12	0.0623	0.1320	0.1320
13	0.0605	0.1402	0.1402
14	0.0598	0.1382	0.1382
15	0.0590	0.1337	0.1337
16	0.0564	0.1348	0.1348
17	0.0568	0.1393	0.1393
18	0.0566	0.1320	0.1320
19	0.0536	0.1321	0.1321
0.0566 0.1320 0.1320
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(64, 5, 5), (64, 5, 5), (64, 5, 5)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 10
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7294	0.6126	0.6126
1	0.5388	0.5031	0.5031
2	0.4285	0.3797	0.3797
3	0.3605	0.3767	0.3767
4	0.3139	0.2932	0.2932
5	0.2811	0.2748	0.2748
6	0.2553	0.2397	0.2397
7	0.2370	0.2186	0.2186
8	0.2184	0.2566	0.2566
9	0.2059	0.2190	0.2190
10	0.1944	0.2187	0.2187
11	0.1867	0.1992	0.1992
12	0.1765	0.2244	0.2244
13	0.1689	0.1801	0.1801
14	0.1648	0.2080	0.2080
15	0.1554	0.1804	0.1804
16	0.1513	0.1722	0.1722
17	0.1452	0.1647	0.1647
18	0.1417	0.1733	0.1733
19	0.1369	0.1732	0.1732
20	0.1336	0.1666	0.1666
21	0.1280	0.1703	0.1703
22	0.1250	0.1676	0.1676
23	0.1220	0.1696	0.1696
24	0.1183	0.1706	0.1706
25	0.1170	0.1528	0.1528
26	0.1142	0.1548	0.1548
27	0.1121	0.1542	0.1542
28	0.1058	0.1576	0.1576
29	0.1039	0.1623	0.1623
30	0.1031	0.1557	0.1557
31	0.1020	0.1521	0.1521
32	0.0994	0.1562	0.1562
33	0.0960	0.1454	0.1454
34	0.0944	0.1600	0.1600
35	0.0908	0.1557	0.1557
36	0.0894	0.1592	0.1592
37	0.0874	0.1414	0.1414
38	0.0846	0.1700	0.1700
39	0.0850	0.1443	0.1443
40	0.0812	0.1590	0.1590
41	0.0806	0.1447	0.1447
42	0.0790	0.1518	0.1518
43	0.0783	0.1377	0.1377
44	0.0772	0.1472	0.1472
45	0.0762	0.1396	0.1396
46	0.0725	0.1389	0.1389
47	0.0705	0.1407	0.1407
48	0.0709	0.1454	0.1454
49	0.0695	0.1425	0.1425
50	0.0694	0.1371	0.1371
51	0.0666	0.1512	0.1512
52	0.0655	0.1350	0.1350
53	0.0641	0.1284	0.1284
54	0.0623	0.1508	0.1508
55	0.0645	0.1400	0.1400
56	0.0630	0.1476	0.1476
57	0.0603	0.1314	0.1314
58	0.0612	0.1361	0.1361
59	0.0581	0.1357	0.1357
60	0.0600	0.1416	0.1416
61	0.0591	0.1360	0.1360
62	0.0560	0.1403	0.1403
63	0.0570	0.1397	0.1397
64	0.0559	0.1420	0.1420
65	0.0427	0.1270	0.1270
66	0.0399	0.1282	0.1282
67	0.0397	0.1240	0.1240
68	0.0383	0.1240	0.1240
69	0.0368	0.1202	0.1202
70	0.0364	0.1221	0.1221
71	0.0346	0.1245	0.1245
72	0.0345	0.1248	0.1248
73	0.0342	0.1298	0.1298
74	0.0337	0.1249	0.1249
75	0.0338	0.1246	0.1246
76	0.0319	0.1233	0.1233
77	0.0315	0.1263	0.1263
78	0.0307	0.1241	0.1241
79	0.0312	0.1252	0.1252
80	0.0299	0.1279	0.1279
81	0.0242	0.1194	0.1194
82	0.0243	0.1184	0.1184
83	0.0231	0.1214	0.1214
84	0.0235	0.1199	0.1199
85	0.0219	0.1211	0.1211
86	0.0222	0.1243	0.1243
87	0.0210	0.1234	0.1234
88	0.0225	0.1172	0.1172
89	0.0200	0.1174	0.1174
90	0.0203	0.1188	0.1188
91	0.0207	0.1186	0.1186
92	0.0202	0.1202	0.1202
93	0.0202	0.1179	0.1179
94	0.0202	0.1207	0.1207
95	0.0200	0.1191	0.1191
96	0.0192	0.1240	0.1240
97	0.0197	0.1180	0.1180
98	0.0190	0.1235	0.1235
99	0.0194	0.1200	0.1200
0.0225 0.1172 0.1172
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(16, 4, 4), (16, 4, 4), (16, 4, 4)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 1
train_on_valid: 1
0	0.7148	0.6220	0.6220
1	0.5841	0.5485	0.5485
2	0.5168	0.4966	0.4966
3	0.4811	0.4733	0.4733
4	0.4571	0.4426	0.4426
5	0.4455	0.4362	0.4362
6	0.4336	0.4060	0.4060
7	0.4249	0.4253	0.4253
8	0.4196	0.4384	0.4384
9	0.4155	0.4079	0.4079
10	0.4112	0.4342	0.4342
11	0.4105	0.4227	0.4227
12	0.4051	0.4169	0.4169
13	0.3897	0.3852	0.3852
14	0.3893	0.4036	0.4036
15	0.3855	0.3955	0.3955
16	0.3837	0.3975	0.3975
17	0.3899	0.3958	0.3958
18	0.3839	0.4010	0.4010
19	0.3829	0.3909	0.3909
0.3897 0.3852 0.3852
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(32, 4, 4), (32, 4, 4), (32, 4, 4)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 1
train_on_valid: 1
0	0.6784	0.5432	0.5432
1	0.5442	0.4859	0.4859
2	0.4820	0.4791	0.4791
3	0.4423	0.4219	0.4219
4	0.4172	0.4219	0.4219
5	0.3956	0.3804	0.3804
6	0.3851	0.3913	0.3913
7	0.3689	0.3789	0.3789
8	0.3574	0.3865	0.3865
9	0.3521	0.3501	0.3501
10	0.3412	0.3752	0.3752
11	0.3329	0.3371	0.3371
12	0.3321	0.3097	0.3097
13	0.3224	0.3257	0.3257
14	0.3200	0.3350	0.3350
15	0.3153	0.3338	0.3338
16	0.3103	0.3239	0.3239
17	0.3041	0.3166	0.3166
18	0.2972	0.3010	0.3010
19	0.2971	0.2872	0.2872
0.2971 0.2872 0.2872
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(8, 6, 6), (8, 6, 6), (8, 6, 6)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7059	0.5525	0.5525
1	0.5043	0.5003	0.5003
2	0.4167	0.3671	0.3671
3	0.3649	0.3479	0.3479
4	0.3327	0.2911	0.2911
5	0.2968	0.3032	0.3032
6	0.2780	0.3152	0.3152
7	0.2569	0.2497	0.2497
8	0.2421	0.2547	0.2547
9	0.2252	0.2380	0.2380
10	0.2141	0.2303	0.2303
11	0.2051	0.2190	0.2190
12	0.1964	0.2155	0.2155
13	0.1903	0.2045	0.2045
14	0.1788	0.2265	0.2265
15	0.1763	0.2149	0.2149
16	0.1685	0.2040	0.2040
17	0.1616	0.1893	0.1893
18	0.1588	0.1915	0.1915
19	0.1565	0.1952	0.1952
0.1616 0.1893 0.1893
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(16, 4, 4), (16, 4, 4), (16, 4, 4)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7131	0.6061	0.6061
1	0.5209	0.4177	0.4177
2	0.4377	0.4209	0.4209
3	0.3894	0.3782	0.3782
4	0.3569	0.3577	0.3577
5	0.3326	0.3390	0.3390
6	0.3151	0.3422	0.3422
7	0.2993	0.2829	0.2829
8	0.2831	0.2863	0.2863
9	0.2715	0.2612	0.2612
10	0.2625	0.2749	0.2749
11	0.2531	0.2599	0.2599
12	0.2434	0.2418	0.2418
13	0.2349	0.2361	0.2361
14	0.2270	0.2435	0.2435
15	0.2207	0.2624	0.2624
16	0.2173	0.2236	0.2236
17	0.2113	0.2386	0.2386
18	0.2058	0.2509	0.2509
19	0.2028	0.2533	0.2533
20	0.1989	0.2446	0.2446
21	0.1955	0.2086	0.2086
22	0.1928	0.2288	0.2288
23	0.1917	0.2140	0.2140
24	0.1880	0.1961	0.1961
25	0.1821	0.2037	0.2037
26	0.1802	0.1966	0.1966
27	0.1780	0.2142	0.2142
28	0.1757	0.1873	0.1873
29	0.1729	0.1899	0.1899
30	0.1740	0.2060	0.2060
31	0.1703	0.1931	0.1931
32	0.1677	0.1889	0.1889
33	0.1656	0.1984	0.1984
34	0.1663	0.1773	0.1773
35	0.1634	0.1878	0.1878
36	0.1616	0.1749	0.1749
37	0.1598	0.2018	0.2018
38	0.1593	0.1868	0.1868
39	0.1572	0.1775	0.1775
40	0.1580	0.1899	0.1899
41	0.1536	0.2072	0.2072
42	0.1519	0.1855	0.1855
43	0.1382	0.1804	0.1804
44	0.1359	0.1770	0.1770
45	0.1343	0.1721	0.1721
46	0.1323	0.1714	0.1714
47	0.1318	0.1754	0.1754
48	0.1304	0.1780	0.1780
49	0.1287	0.1710	0.1710
50	0.1294	0.1681	0.1681
51	0.1285	0.1643	0.1643
52	0.1273	0.1710	0.1710
53	0.1258	0.1637	0.1637
54	0.1253	0.1700	0.1700
55	0.1260	0.1721	0.1721
56	0.1251	0.1692	0.1692
57	0.1259	0.1720	0.1720
58	0.1254	0.1721	0.1721
59	0.1236	0.1661	0.1661
60	0.1154	0.1641	0.1641
61	0.1140	0.1616	0.1616
62	0.1143	0.1605	0.1605
63	0.1129	0.1635	0.1635
64	0.1134	0.1589	0.1589
65	0.1138	0.1628	0.1628
66	0.1137	0.1610	0.1610
67	0.1140	0.1636	0.1636
68	0.1121	0.1616	0.1616
69	0.1117	0.1621	0.1621
70	0.1116	0.1656	0.1656
71	0.1082	0.1596	0.1596
72	0.1076	0.1583	0.1583
73	0.1065	0.1595	0.1595
74	0.1060	0.1608	0.1608
75	0.1062	0.1583	0.1583
76	0.1050	0.1602	0.1602
77	0.1059	0.1610	0.1610
78	0.1066	0.1613	0.1613
79	0.1053	0.1593	0.1593
80	0.1049	0.1609	0.1609
81	0.1049	0.1595	0.1595
82	0.1029	0.1580	0.1580
83	0.1025	0.1579	0.1579
84	0.1025	0.1605	0.1605
85	0.1022	0.1582	0.1582
86	0.1023	0.1577	0.1577
87	0.1022	0.1579	0.1579
88	0.1018	0.1613	0.1613
89	0.1023	0.1615	0.1615
90	0.1012	0.1594	0.1594
91	0.1012	0.1583	0.1583
92	0.1016	0.1580	0.1580
93	0.1006	0.1591	0.1591
94	0.1005	0.1582	0.1582
95	0.1003	0.1599	0.1599
96	0.1016	0.1599	0.1599
97	0.1009	0.1579	0.1579
98	0.1008	0.1613	0.1613
99	0.1006	0.1602	0.1602
0.1023 0.1577 0.1577
train_epochs: 20
save_model: model_saved.npz
filter_shape: [(1, 18, 18), (1, 18, 18), (1, 18, 18)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7741	0.7305	0.7305
1	0.6175	0.5172	0.5172
2	0.4774	0.4164	0.4164
3	0.4277	0.3949	0.3949
4	0.3767	0.3319	0.3319
5	0.3523	0.3356	0.3356
6	0.3163	0.2907	0.2907
7	0.2916	0.2974	0.2974
8	0.2677	0.2555	0.2555
9	0.2455	0.2708	0.2708
10	0.2300	0.2206	0.2206
11	0.2156	0.2246	0.2246
12	0.2005	0.2354	0.2354
13	0.1912	0.2001	0.2001
14	0.1798	0.2222	0.2222
15	0.1717	0.2175	0.2175
16	0.1610	0.1775	0.1775
17	0.1547	0.2030	0.2030
18	0.1475	0.1996	0.1996
19	0.1377	0.1665	0.1665
0.1377 0.1665 0.1665
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(32, 4, 4), (32, 4, 4), (32, 4, 4)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.6925	0.5104	0.5104
1	0.4901	0.4718	0.4718
2	0.4110	0.3592	0.3592
3	0.3642	0.3683	0.3683
4	0.3216	0.3347	0.3347
5	0.2951	0.2759	0.2759
6	0.2682	0.2736	0.2736
7	0.2488	0.2425	0.2425
8	0.2271	0.2357	0.2357
9	0.2118	0.2053	0.2053
10	0.2008	0.2561	0.2561
11	0.1856	0.1930	0.1930
12	0.1794	0.1928	0.1928
13	0.1722	0.1949	0.1949
14	0.1626	0.1780	0.1780
15	0.1553	0.1754	0.1754
16	0.1509	0.1756	0.1756
17	0.1443	0.1760	0.1760
18	0.1410	0.1840	0.1840
19	0.1339	0.1611	0.1611
20	0.1280	0.1845	0.1845
21	0.1263	0.1741	0.1741
22	0.1185	0.1700	0.1700
23	0.1174	0.1603	0.1603
24	0.1115	0.1713	0.1713
25	0.1095	0.1599	0.1599
26	0.1050	0.1572	0.1572
27	0.1030	0.1631	0.1631
28	0.1035	0.1631	0.1631
29	0.0969	0.1525	0.1525
30	0.0966	0.1627	0.1627
31	0.0938	0.1399	0.1399
32	0.0903	0.1522	0.1522
33	0.0896	0.1479	0.1479
34	0.0858	0.1592	0.1592
35	0.0859	0.1479	0.1479
36	0.0827	0.1698	0.1698
37	0.0797	0.1428	0.1428
38	0.0587	0.1455	0.1455
39	0.0562	0.1359	0.1359
40	0.0537	0.1391	0.1391
41	0.0522	0.1349	0.1349
42	0.0516	0.1373	0.1373
43	0.0515	0.1410	0.1410
44	0.0494	0.1307	0.1307
45	0.0474	0.1365	0.1365
46	0.0486	0.1277	0.1277
47	0.0469	0.1303	0.1303
48	0.0460	0.1306	0.1306
49	0.0437	0.1289	0.1289
50	0.0430	0.1324	0.1324
51	0.0430	0.1244	0.1244
52	0.0434	0.1347	0.1347
53	0.0416	0.1246	0.1246
54	0.0410	0.1347	0.1347
55	0.0421	0.1269	0.1269
56	0.0400	0.1276	0.1276
57	0.0389	0.1366	0.1366
58	0.0304	0.1274	0.1274
59	0.0289	0.1224	0.1224
60	0.0287	0.1223	0.1223
61	0.0276	0.1251	0.1251
62	0.0274	0.1244	0.1244
63	0.0267	0.1283	0.1283
64	0.0255	0.1250	0.1250
65	0.0260	0.1232	0.1232
66	0.0253	0.1217	0.1217
67	0.0232	0.1240	0.1240
68	0.0241	0.1249	0.1249
69	0.0236	0.1231	0.1231
70	0.0237	0.1267	0.1267
71	0.0233	0.1270	0.1270
72	0.0221	0.1270	0.1270
73	0.0196	0.1221	0.1221
74	0.0187	0.1193	0.1193
75	0.0186	0.1231	0.1231
76	0.0175	0.1209	0.1209
77	0.0178	0.1213	0.1213
78	0.0172	0.1216	0.1216
79	0.0176	0.1250	0.1250
80	0.0172	0.1218	0.1218
81	0.0160	0.1193	0.1193
82	0.0155	0.1198	0.1198
83	0.0154	0.1205	0.1205
84	0.0148	0.1205	0.1205
85	0.0143	0.1215	0.1215
86	0.0153	0.1210	0.1210
87	0.0149	0.1221	0.1221
88	0.0142	0.1203	0.1203
89	0.0140	0.1206	0.1206
90	0.0142	0.1202	0.1202
91	0.0137	0.1193	0.1193
92	0.0135	0.1206	0.1206
93	0.0137	0.1199	0.1199
94	0.0136	0.1195	0.1195
95	0.0134	0.1216	0.1216
96	0.0135	0.1204	0.1204
97	0.0133	0.1196	0.1196
98	0.0127	0.1208	0.1208
99	0.0132	0.1212	0.1212
0.0137 0.1193 0.1193
train_epochs: 10
save_model: model_saved.npz
filter_shape: [(16, 6, 6), (16, 6, 6), (16, 6, 6)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 1
train_on_valid: 1
0	0.6805	0.5754	0.5754
1	0.5388	0.5090	0.5090
2	0.4769	0.5137	0.5137
3	0.4528	0.4154	0.4154
4	0.4307	0.4331	0.4331
5	0.4170	0.4023	0.4023
6	0.4087	0.4134	0.4134
7	0.4002	0.4245	0.4245
8	0.3927	0.3768	0.3768
9	0.3883	0.4449	0.4449
0.3927 0.3768 0.3768
train_epochs: 10
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
load_model: 0
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 0
pool_size: 2
n_nested: 1
train_on_valid: 1
0	0.7098	0.6011	0.6011
1	0.5792	0.5502	0.5502
2	0.5091	0.4886	0.4886
3	0.4761	0.4839	0.4839
4	0.4431	0.4003	0.4003
5	0.4255	0.4262	0.4262
6	0.4119	0.4072	0.4072
7	0.3945	0.3801	0.3801
8	0.3856	0.3726	0.3726
9	0.3789	0.3810	0.3810
0.3856 0.3726 0.3726
load_model: 0
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(48, 3, 3), (48, 3, 3), (48, 3, 3)]
batch_size: 200
dataset: cifar10
kernel_pool_size: (1, 1)
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 0
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7183	0.5954	0.5954
1	0.5241	0.5102	0.5102
2	0.4329	0.4235	0.4235
3	0.3899	0.3490	0.3490
4	0.3594	0.3876	0.3876
5	0.3372	0.3473	0.3473
6	0.3221	0.3083	0.3083
7	0.3046	0.2838	0.2838
8	0.2878	0.2944	0.2944
9	0.2745	0.3312	0.3312
10	0.2606	0.2627	0.2627
11	0.2478	0.2586	0.2586
12	0.2395	0.2423	0.2423
13	0.2315	0.2483	0.2483
14	0.2249	0.2427	0.2427
15	0.2175	0.2154	0.2154
16	0.2085	0.2527	0.2527
17	0.2042	0.2242	0.2242
18	0.1989	0.2126	0.2126
19	0.1940	0.2179	0.2179
20	0.1877	0.2089	0.2089
21	0.1898	0.2123	0.2123
22	0.1822	0.1924	0.1924
23	0.1777	0.1963	0.1963
24	0.1764	0.2020	0.2020
25	0.1739	0.2050	0.2050
26	0.1716	0.2016	0.2016
27	0.1683	0.2066	0.2066
28	0.1623	0.1877	0.1877
29	0.1618	0.1940	0.1940
30	0.1614	0.1916	0.1916
31	0.1606	0.1882	0.1882
32	0.1568	0.1965	0.1965
33	0.1567	0.1841	0.1841
34	0.1547	0.1898	0.1898
35	0.1548	0.2046	0.2046
36	0.1504	0.1895	0.1895
37	0.1485	0.1933	0.1933
38	0.1460	0.1843	0.1843
39	0.1456	0.1836	0.1836
40	0.1463	0.1896	0.1896
41	0.1454	0.1847	0.1847
42	0.1428	0.1756	0.1756
43	0.1428	0.1831	0.1831
44	0.1408	0.1839	0.1839
45	0.1409	0.1878	0.1878
46	0.1382	0.1906	0.1906
47	0.1378	0.1712	0.1712
48	0.1368	0.1750	0.1750
49	0.1361	0.1672	0.1672
50	0.1333	0.1839	0.1839
51	0.1313	0.1845	0.1845
52	0.1335	0.1671	0.1671
53	0.1293	0.1737	0.1737
54	0.1287	0.1726	0.1726
55	0.1315	0.1871	0.1871
56	0.1276	0.1754	0.1754
57	0.1265	0.1917	0.1917
58	0.1271	0.1849	0.1849
59	0.1128	0.1649	0.1649
60	0.1098	0.1511	0.1511
61	0.1090	0.1622	0.1622
62	0.1086	0.1625	0.1625
63	0.1083	0.1560	0.1560
64	0.1057	0.1591	0.1591
65	0.1071	0.1551	0.1551
66	0.1060	0.1585	0.1585
67	0.1008	0.1509	0.1509
68	0.0987	0.1558	0.1558
69	0.0988	0.1483	0.1483
70	0.0976	0.1522	0.1522
71	0.0966	0.1485	0.1485
72	0.0984	0.1517	0.1517
73	0.0971	0.1550	0.1550
74	0.0954	0.1549	0.1549
75	0.0962	0.1505	0.1505
76	0.0933	0.1517	0.1517
77	0.0930	0.1502	0.1502
78	0.0924	0.1506	0.1506
79	0.0919	0.1509	0.1509
80	0.0915	0.1502	0.1502
81	0.0911	0.1492	0.1492
82	0.0890	0.1502	0.1502
83	0.0901	0.1474	0.1474
84	0.0896	0.1512	0.1512
85	0.0889	0.1500	0.1500
86	0.0888	0.1487	0.1487
87	0.0894	0.1512	0.1512
88	0.0894	0.1506	0.1506
89	0.0887	0.1505	0.1505
90	0.0883	0.1470	0.1470
91	0.0886	0.1465	0.1465
92	0.0877	0.1471	0.1471
93	0.0875	0.1488	0.1488
94	0.0876	0.1484	0.1484
95	0.0871	0.1496	0.1496
96	0.0880	0.1482	0.1482
97	0.0882	0.1494	0.1494
98	0.0871	0.1498	0.1498
99	0.0875	0.1483	0.1483
0.0886 0.1465 0.1465
load_model: 0
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(64, 3, 3), (64, 3, 3), (64, 3, 3)]
batch_size: 200
dataset: cifar10
kernel_pool_size: 1
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 0
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7011	0.5446	0.5446
1	0.4954	0.4035	0.4035
2	0.4108	0.4616	0.4616
3	0.3686	0.3362	0.3362
4	0.3320	0.3145	0.3145
5	0.3043	0.2831	0.2831
6	0.2793	0.2488	0.2488
7	0.2588	0.2689	0.2689
8	0.2407	0.2534	0.2534
9	0.2283	0.2465	0.2465
10	0.2170	0.2471	0.2471
11	0.2054	0.2102	0.2102
12	0.1960	0.2121	0.2121
13	0.1889	0.2324	0.2324
14	0.1820	0.2142	0.2142
15	0.1751	0.1987	0.1987
16	0.1731	0.1914	0.1914
17	0.1648	0.2119	0.2119
18	0.1623	0.1780	0.1780
19	0.1562	0.1844	0.1844
20	0.1553	0.1741	0.1741
21	0.1496	0.1906	0.1906
22	0.1446	0.1994	0.1994
23	0.1425	0.1911	0.1911
24	0.1378	0.1991	0.1991
25	0.1352	0.1717	0.1717
26	0.1343	0.1742	0.1742
27	0.1316	0.1784	0.1784
28	0.1277	0.1550	0.1550
29	0.1259	0.1708	0.1708
30	0.1266	0.1854	0.1854
31	0.1235	0.1810	0.1810
32	0.1217	0.1648	0.1648
33	0.1183	0.1715	0.1715
34	0.1167	0.1714	0.1714
35	0.0991	0.1474	0.1474
36	0.0957	0.1504	0.1504
37	0.0948	0.1505	0.1505
38	0.0936	0.1513	0.1513
39	0.0925	0.1489	0.1489
40	0.0899	0.1506	0.1506
41	0.0901	0.1486	0.1486
42	0.0822	0.1430	0.1430
43	0.0795	0.1450	0.1450
44	0.0792	0.1433	0.1433
45	0.0791	0.1399	0.1399
46	0.0791	0.1434	0.1434
47	0.0783	0.1437	0.1437
48	0.0776	0.1401	0.1401
49	0.0765	0.1381	0.1381
50	0.0757	0.1361	0.1361
51	0.0768	0.1373	0.1373
52	0.0736	0.1428	0.1428
53	0.0741	0.1430	0.1430
54	0.0725	0.1387	0.1387
55	0.0726	0.1430	0.1430
56	0.0720	0.1412	0.1412
57	0.0692	0.1412	0.1412
58	0.0670	0.1366	0.1366
59	0.0667	0.1378	0.1378
60	0.0657	0.1388	0.1388
61	0.0660	0.1369	0.1369
62	0.0654	0.1402	0.1402
63	0.0649	0.1369	0.1369
64	0.0637	0.1343	0.1343
65	0.0634	0.1352	0.1352
66	0.0630	0.1359	0.1359
67	0.0626	0.1341	0.1341
68	0.0630	0.1338	0.1338
69	0.0623	0.1338	0.1338
70	0.0622	0.1365	0.1365
71	0.0616	0.1345	0.1345
72	0.0618	0.1355	0.1355
73	0.0613	0.1365	0.1365
74	0.0612	0.1346	0.1346
75	0.0613	0.1338	0.1338
76	0.0616	0.1342	0.1342
77	0.0615	0.1333	0.1333
78	0.0604	0.1343	0.1343
79	0.0607	0.1357	0.1357
80	0.0607	0.1353	0.1353
81	0.0605	0.1360	0.1360
82	0.0598	0.1361	0.1361
83	0.0602	0.1355	0.1355
84	0.0595	0.1336	0.1336
85	0.0592	0.1357	0.1357
86	0.0594	0.1365	0.1365
87	0.0589	0.1363	0.1363
88	0.0595	0.1365	0.1365
89	0.0595	0.1347	0.1347
90	0.0589	0.1374	0.1374
91	0.0587	0.1371	0.1371
92	0.0584	0.1365	0.1365
93	0.0589	0.1345	0.1345
94	0.0580	0.1348	0.1348
95	0.0585	0.1337	0.1337
96	0.0583	0.1377	0.1377
97	0.0581	0.1362	0.1362
98	0.0580	0.1359	0.1359
99	0.0577	0.1364	0.1364
0.0615 0.1333 0.1333
load_model: 0
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(32, 4, 4), (32, 4, 4), (32, 4, 4)]
batch_size: 200
dataset: cifar10
kernel_pool_size: 2
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7719	0.6257	0.6257
1	0.6032	0.5140	0.5140
2	0.4856	0.4815	0.4815
3	0.4291	0.4092	0.4092
4	0.3857	0.4446	0.4446
5	0.3636	0.3214	0.3214
6	0.3431	0.3840	0.3840
7	0.3286	0.3004	0.3004
8	0.3173	0.3350	0.3350
9	0.3028	0.2960	0.2960
10	0.2941	0.3053	0.3053
11	0.2832	0.2728	0.2728
12	0.2764	0.2877	0.2877
13	0.2685	0.2805	0.2805
14	0.2594	0.2565	0.2565
15	0.2536	0.2596	0.2596
16	0.2488	0.2873	0.2873
17	0.2427	0.2669	0.2669
18	0.2362	0.2413	0.2413
19	0.2313	0.2398	0.2398
20	0.2260	0.2306	0.2306
21	0.2208	0.2423	0.2423
22	0.2214	0.2398	0.2398
23	0.2150	0.2362	0.2362
24	0.2124	0.2518	0.2518
25	0.2094	0.2244	0.2244
26	0.2080	0.2345	0.2345
27	0.2036	0.2399	0.2399
28	0.2014	0.2202	0.2202
29	0.1990	0.2309	0.2309
30	0.1976	0.2116	0.2116
31	0.1905	0.2172	0.2172
32	0.1907	0.2151	0.2151
33	0.1889	0.2166	0.2166
34	0.1880	0.2259	0.2259
35	0.1873	0.2185	0.2185
36	0.1835	0.2162	0.2162
37	0.1652	0.2027	0.2027
38	0.1611	0.1988	0.1988
39	0.1592	0.1968	0.1968
40	0.1570	0.2035	0.2035
41	0.1573	0.1912	0.1912
42	0.1567	0.1855	0.1855
43	0.1538	0.1904	0.1904
44	0.1542	0.1974	0.1974
45	0.1539	0.1917	0.1917
46	0.1525	0.1975	0.1975
47	0.1496	0.1919	0.1919
48	0.1520	0.1840	0.1840
49	0.1515	0.1932	0.1932
50	0.1516	0.1851	0.1851
51	0.1506	0.1893	0.1893
52	0.1496	0.1821	0.1821
53	0.1480	0.1858	0.1858
54	0.1488	0.1808	0.1808
55	0.1455	0.1801	0.1801
56	0.1460	0.1847	0.1847
57	0.1453	0.1804	0.1804
58	0.1439	0.1867	0.1867
59	0.1437	0.1901	0.1901
60	0.1424	0.1846	0.1846
61	0.1426	0.1821	0.1821
62	0.1334	0.1802	0.1802
63	0.1311	0.1756	0.1756
64	0.1321	0.1799	0.1799
65	0.1299	0.1753	0.1753
66	0.1297	0.1781	0.1781
67	0.1292	0.1802	0.1802
68	0.1305	0.1776	0.1776
69	0.1300	0.1728	0.1728
70	0.1302	0.1786	0.1786
71	0.1291	0.1807	0.1807
72	0.1289	0.1768	0.1768
73	0.1281	0.1763	0.1763
74	0.1276	0.1790	0.1790
75	0.1267	0.1739	0.1739
76	0.1236	0.1771	0.1771
77	0.1230	0.1729	0.1729
78	0.1220	0.1767	0.1767
79	0.1220	0.1752	0.1752
80	0.1228	0.1747	0.1747
81	0.1215	0.1745	0.1745
82	0.1193	0.1686	0.1686
83	0.1191	0.1700	0.1700
84	0.1192	0.1709	0.1709
85	0.1181	0.1694	0.1694
86	0.1186	0.1703	0.1703
87	0.1184	0.1720	0.1720
88	0.1187	0.1715	0.1715
89	0.1172	0.1710	0.1710
90	0.1171	0.1721	0.1721
91	0.1168	0.1712	0.1712
92	0.1167	0.1713	0.1713
93	0.1165	0.1692	0.1692
94	0.1163	0.1696	0.1696
95	0.1169	0.1701	0.1701
96	0.1167	0.1698	0.1698
97	0.1162	0.1701	0.1701
98	0.1158	0.1723	0.1723
99	0.1161	0.1703	0.1703
0.1193 0.1686 0.1686
load_model: 0
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(1, 18, 18), (1, 18, 18), (1, 18, 18)]
batch_size: 200
dataset: cifar10
kernel_pool_size: 2
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7377	0.6442	0.6442
1	0.5368	0.6354	0.6354
2	0.4467	0.4260	0.4260
3	0.3907	0.3645	0.3645
4	0.3540	0.3242	0.3242
5	0.3261	0.3601	0.3601
6	0.3069	0.2950	0.2950
7	0.2938	0.2655	0.2655
8	0.2820	0.3007	0.3007
9	0.2642	0.2921	0.2921
10	0.2566	0.2919	0.2919
11	0.2485	0.2510	0.2510
12	0.2390	0.2350	0.2350
13	0.2321	0.2461	0.2461
14	0.2221	0.2248	0.2248
15	0.2208	0.2403	0.2403
16	0.2142	0.2488	0.2488
17	0.2098	0.2225	0.2225
18	0.2080	0.2287	0.2287
19	0.2016	0.2203	0.2203
20	0.1992	0.2181	0.2181
21	0.1977	0.2111	0.2111
22	0.1896	0.1995	0.1995
23	0.1858	0.2201	0.2201
24	0.1884	0.1966	0.1966
25	0.1831	0.2012	0.2012
26	0.1809	0.2013	0.2013
27	0.1768	0.2202	0.2202
28	0.1728	0.2422	0.2422
29	0.1712	0.2089	0.2089
30	0.1714	0.1985	0.1985
31	0.1519	0.1857	0.1857
32	0.1457	0.1888	0.1888
33	0.1449	0.2115	0.2115
34	0.1461	0.1970	0.1970
35	0.1445	0.1834	0.1834
36	0.1426	0.1793	0.1793
37	0.1426	0.1836	0.1836
38	0.1374	0.1767	0.1767
39	0.1398	0.1894	0.1894
40	0.1366	0.1897	0.1897
41	0.1357	0.1696	0.1696
42	0.1348	0.1823	0.1823
43	0.1339	0.1725	0.1725
44	0.1354	0.1848	0.1848
45	0.1340	0.1796	0.1796
46	0.1330	0.1751	0.1751
47	0.1312	0.1811	0.1811
48	0.1235	0.1820	0.1820
49	0.1219	0.1726	0.1726
50	0.1221	0.1715	0.1715
51	0.1209	0.1797	0.1797
52	0.1210	0.1740	0.1740
53	0.1175	0.1669	0.1669
54	0.1179	0.1683	0.1683
55	0.1186	0.1716	0.1716
56	0.1160	0.1814	0.1814
57	0.1150	0.1791	0.1791
58	0.1169	0.1698	0.1698
59	0.1155	0.1648	0.1648
60	0.1144	0.1646	0.1646
61	0.1144	0.1599	0.1599
62	0.1135	0.1709	0.1709
63	0.1147	0.1646	0.1646
64	0.1122	0.1702	0.1702
65	0.1118	0.1655	0.1655
66	0.1122	0.1662	0.1662
67	0.1132	0.1717	0.1717
68	0.1075	0.1631	0.1631
69	0.1063	0.1685	0.1685
70	0.1056	0.1644	0.1644
71	0.1056	0.1606	0.1606
72	0.1036	0.1689	0.1689
73	0.1033	0.1656	0.1656
74	0.1028	0.1635	0.1635
75	0.1018	0.1620	0.1620
76	0.1004	0.1626	0.1626
77	0.1017	0.1608	0.1608
78	0.1004	0.1650	0.1650
79	0.1006	0.1619	0.1619
80	0.0993	0.1614	0.1614
81	0.0997	0.1624	0.1624
82	0.0992	0.1626	0.1626
83	0.0993	0.1616	0.1616
84	0.0998	0.1622	0.1622
85	0.0988	0.1625	0.1625
86	0.0988	0.1626	0.1626
87	0.0981	0.1644	0.1644
88	0.0988	0.1639	0.1639
89	0.0995	0.1642	0.1642
90	0.0990	0.1628	0.1628
91	0.0985	0.1630	0.1630
92	0.0984	0.1618	0.1618
93	0.0980	0.1628	0.1628
94	0.0986	0.1617	0.1617
95	0.0977	0.1623	0.1623
96	0.0980	0.1622	0.1622
97	0.0978	0.1615	0.1615
98	0.0971	0.1628	0.1628
99	0.0975	0.1615	0.1615
0.1144 0.1599 0.1599
load_model: 0
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(128, 3, 3), (128, 3, 3), (128, 3, 3)]
batch_size: 200
dataset: cifar10
kernel_pool_size: 1
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 0
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.6804	0.5570	0.5570
1	0.4816	0.5636	0.5636
2	0.3991	0.3362	0.3362
3	0.3395	0.2840	0.2840
4	0.2957	0.3082	0.3082
5	0.2613	0.2656	0.2656
6	0.2289	0.2463	0.2463
7	0.2064	0.2048	0.2048
8	0.1886	0.2142	0.2142
9	0.1764	0.2007	0.2007
10	0.1649	0.1921	0.1921
11	0.1499	0.1860	0.1860
12	0.1406	0.1846	0.1846
13	0.1295	0.1731	0.1731
14	0.1215	0.1643	0.1643
15	0.1134	0.1920	0.1920
16	0.1088	0.1486	0.1486
17	0.0980	0.1581	0.1581
18	0.0962	0.1401	0.1401
19	0.0929	0.1516	0.1516
20	0.0847	0.1417	0.1417
21	0.0803	0.1395	0.1395
22	0.0751	0.1435	0.1435
23	0.0717	0.1317	0.1317
24	0.0693	0.1596	0.1596
25	0.0673	0.1486	0.1486
26	0.0612	0.1354	0.1354
27	0.0574	0.1309	0.1309
28	0.0562	0.1384	0.1384
29	0.0528	0.1270	0.1270
30	0.0495	0.1461	0.1461
31	0.0497	0.1452	0.1452
32	0.0459	0.1379	0.1379
33	0.0433	0.1193	0.1193
34	0.0423	0.1357	0.1357
35	0.0378	0.1255	0.1255
36	0.0389	0.1203	0.1203
37	0.0370	0.1180	0.1180
38	0.0360	0.1192	0.1192
39	0.0330	0.1221	0.1221
40	0.0311	0.1139	0.1139
41	0.0297	0.1200	0.1200
42	0.0285	0.1282	0.1282
43	0.0267	0.1236	0.1236
44	0.0261	0.1271	0.1271
45	0.0249	0.1195	0.1195
46	0.0241	0.1206	0.1206
47	0.0121	0.1110	0.1110
48	0.0101	0.1079	0.1079
49	0.0095	0.1095	0.1095
50	0.0086	0.1113	0.1113
51	0.0074	0.1116	0.1116
52	0.0066	0.1114	0.1114
53	0.0063	0.1154	0.1154
54	0.0067	0.1094	0.1094
55	0.0036	0.1053	0.1053
56	0.0033	0.1065	0.1065
57	0.0028	0.1052	0.1052
58	0.0026	0.1059	0.1059
59	0.0026	0.1073	0.1073
60	0.0026	0.1113	0.1113
61	0.0026	0.1082	0.1082
62	0.0022	0.1087	0.1087
63	0.0018	0.1073	0.1073
64	0.0016	0.1065	0.1065
65	0.0011	0.1059	0.1059
66	0.0010	0.1054	0.1054
67	0.0012	0.1063	0.1063
68	0.0009	0.1056	0.1056
69	0.0010	0.1057	0.1057
70	0.0008	0.1062	0.1062
71	0.0009	0.1057	0.1057
72	0.0007	0.1049	0.1049
73	0.0007	0.1046	0.1046
74	0.0007	0.1055	0.1055
75	0.0008	0.1066	0.1066
76	0.0007	0.1056	0.1056
77	0.0006	0.1056	0.1056
78	0.0006	0.1038	0.1038
79	0.0004	0.1052	0.1052
80	0.0004	0.1052	0.1052
81	0.0004	0.1060	0.1060
82	0.0004	0.1056	0.1056
83	0.0005	0.1043	0.1043
84	0.0004	0.1036	0.1036
85	0.0006	0.1045	0.1045
86	0.0005	0.1049	0.1049
87	0.0005	0.1030	0.1030
88	0.0004	0.1056	0.1056
89	0.0005	0.1043	0.1043
90	0.0004	0.1053	0.1053
91	0.0005	0.1057	0.1057
92	0.0003	0.1053	0.1053
93	0.0003	0.1056	0.1056
94	0.0003	0.1061	0.1061
95	0.0003	0.1054	0.1054
96	0.0002	0.1034	0.1034
97	0.0003	0.1039	0.1039
98	0.0002	0.1041	0.1041
99	0.0002	0.1042	0.1042
0.0005 0.1030 0.1030
load_model: 0
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(144, 3, 3), (144, 3, 3), (144, 3, 3)]
batch_size: 200
dataset: cifar10
kernel_pool_size: (1, 1)
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 0
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.6818	0.5189	0.5189
1	0.4783	0.3874	0.3874
2	0.3973	0.3810	0.3810
3	0.3382	0.2969	0.2969
4	0.2879	0.2584	0.2584
5	0.2517	0.2539	0.2539
6	0.2270	0.1991	0.1991
7	0.2055	0.2269	0.2269
8	0.1841	0.1918	0.1918
9	0.1678	0.1855	0.1855
10	0.1554	0.1606	0.1606
11	0.1432	0.1819	0.1819
12	0.1335	0.1616	0.1616
13	0.1242	0.1531	0.1531
14	0.1151	0.1456	0.1456
15	0.1064	0.1487	0.1487
16	0.1013	0.1390	0.1390
17	0.0918	0.1596	0.1596
18	0.0873	0.1449	0.1449
19	0.0807	0.1417	0.1417
20	0.0773	0.1363	0.1363
21	0.0738	0.1291	0.1291
22	0.0680	0.1399	0.1399
23	0.0642	0.1446	0.1446
24	0.0589	0.1438	0.1438
25	0.0546	0.1298	0.1298
26	0.0552	0.1309	0.1309
27	0.0492	0.1374	0.1374
28	0.0290	0.1140	0.1140
29	0.0245	0.1126	0.1126
30	0.0225	0.1145	0.1145
31	0.0209	0.1164	0.1164
32	0.0201	0.1180	0.1180
33	0.0187	0.1208	0.1208
34	0.0183	0.1136	0.1136
35	0.0160	0.1150	0.1150
36	0.0101	0.1059	0.1059
37	0.0082	0.1081	0.1081
38	0.0075	0.1066	0.1066
39	0.0073	0.1063	0.1063
40	0.0072	0.1043	0.1043
41	0.0068	0.1038	0.1038
42	0.0058	0.1114	0.1114
43	0.0056	0.1048	0.1048
44	0.0052	0.1080	0.1080
45	0.0047	0.1039	0.1039
46	0.0042	0.1071	0.1071
47	0.0044	0.1055	0.1055
48	0.0029	0.1059	0.1059
49	0.0027	0.1049	0.1049
50	0.0025	0.1065	0.1065
51	0.0023	0.1055	0.1055
52	0.0024	0.1037	0.1037
53	0.0020	0.1051	0.1051
54	0.0021	0.1046	0.1046
55	0.0020	0.1046	0.1046
56	0.0020	0.1038	0.1038
57	0.0020	0.1054	0.1054
58	0.0017	0.1038	0.1038
59	0.0013	0.1034	0.1034
60	0.0012	0.1027	0.1027
61	0.0013	0.1031	0.1031
62	0.0013	0.1025	0.1025
63	0.0013	0.1035	0.1035
64	0.0010	0.1037	0.1037
65	0.0010	0.1026	0.1026
66	0.0011	0.1031	0.1031
67	0.0009	0.1041	0.1041
68	0.0011	0.1033	0.1033
69	0.0010	0.1025	0.1025
70	0.0009	0.1027	0.1027
71	0.0008	0.1012	0.1012
72	0.0008	0.1018	0.1018
73	0.0009	0.1034	0.1034
74	0.0007	0.1030	0.1030
75	0.0007	0.1026	0.1026
76	0.0007	0.1032	0.1032
77	0.0007	0.1031	0.1031
78	0.0007	0.1025	0.1025
79	0.0006	0.1029	0.1029
80	0.0005	0.1029	0.1029
81	0.0006	0.1041	0.1041
82	0.0005	0.1019	0.1019
83	0.0006	0.1025	0.1025
84	0.0006	0.1023	0.1023
85	0.0006	0.1024	0.1024
86	0.0006	0.1022	0.1022
87	0.0006	0.1023	0.1023
88	0.0006	0.1029	0.1029
89	0.0006	0.1027	0.1027
90	0.0005	0.1030	0.1030
91	0.0005	0.1028	0.1028
92	0.0005	0.1024	0.1024
93	0.0005	0.1034	0.1034
94	0.0005	0.1025	0.1025
95	0.0005	0.1027	0.1027
96	0.0004	0.1026	0.1026
97	0.0005	0.1021	0.1021
98	0.0005	0.1031	0.1031
99	0.0005	0.1029	0.1029
0.0008 0.1012 0.1012
load_model: 0
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(16, 5, 5), (16, 5, 5), (16, 5, 5)]
batch_size: 200
dataset: cifar10
kernel_pool_size: (1, 1)
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7141	0.6400	0.6400
1	0.5102	0.4291	0.4291
2	0.4222	0.3895	0.3895
3	0.3661	0.3739	0.3739
4	0.3329	0.2918	0.2918
5	0.3050	0.3064	0.3064
6	0.2784	0.2649	0.2649
7	0.2514	0.2426	0.2426
8	0.2359	0.2408	0.2408
9	0.2176	0.2187	0.2187
10	0.2073	0.2249	0.2249
11	0.1974	0.2132	0.2132
12	0.1860	0.2254	0.2254
13	0.1772	0.2206	0.2206
14	0.1708	0.1960	0.1960
15	0.1597	0.1865	0.1865
16	0.1580	0.1995	0.1995
17	0.1495	0.2036	0.2036
18	0.1463	0.1815	0.1815
19	0.1391	0.1858	0.1858
20	0.1361	0.1651	0.1651
21	0.1315	0.1945	0.1945
22	0.1295	0.1753	0.1753
23	0.1227	0.1663	0.1663
24	0.1217	0.1748	0.1748
25	0.1185	0.1685	0.1685
26	0.1122	0.1729	0.1729
27	0.0882	0.1466	0.1466
28	0.0847	0.1509	0.1509
29	0.0838	0.1524	0.1524
30	0.0803	0.1499	0.1499
31	0.0802	0.1581	0.1581
32	0.0780	0.1391	0.1391
33	0.0755	0.1474	0.1474
34	0.0739	0.1429	0.1429
35	0.0723	0.1436	0.1436
36	0.0706	0.1453	0.1453
37	0.0707	0.1453	0.1453
38	0.0700	0.1445	0.1445
39	0.0588	0.1426	0.1426
40	0.0575	0.1385	0.1385
41	0.0543	0.1389	0.1389
42	0.0546	0.1381	0.1381
43	0.0536	0.1331	0.1331
44	0.0523	0.1332	0.1332
45	0.0535	0.1349	0.1349
46	0.0528	0.1374	0.1374
47	0.0506	0.1349	0.1349
48	0.0493	0.1362	0.1362
49	0.0479	0.1367	0.1367
50	0.0444	0.1340	0.1340
51	0.0430	0.1306	0.1306
52	0.0431	0.1336	0.1336
53	0.0423	0.1274	0.1274
54	0.0420	0.1282	0.1282
55	0.0413	0.1278	0.1278
56	0.0412	0.1306	0.1306
57	0.0409	0.1344	0.1344
58	0.0398	0.1314	0.1314
59	0.0398	0.1345	0.1345
60	0.0375	0.1314	0.1314
61	0.0366	0.1336	0.1336
62	0.0363	0.1293	0.1293
63	0.0363	0.1325	0.1325
64	0.0363	0.1307	0.1307
65	0.0360	0.1276	0.1276
66	0.0350	0.1282	0.1282
67	0.0345	0.1298	0.1298
68	0.0344	0.1289	0.1289
69	0.0339	0.1288	0.1288
70	0.0332	0.1297	0.1297
71	0.0338	0.1302	0.1302
72	0.0336	0.1300	0.1300
73	0.0328	0.1301	0.1301
74	0.0336	0.1306	0.1306
75	0.0327	0.1298	0.1298
76	0.0330	0.1284	0.1284
77	0.0329	0.1310	0.1310
78	0.0326	0.1282	0.1282
79	0.0326	0.1284	0.1284
80	0.0322	0.1286	0.1286
81	0.0321	0.1286	0.1286
82	0.0321	0.1290	0.1290
83	0.0325	0.1285	0.1285
84	0.0323	0.1302	0.1302
85	0.0320	0.1294	0.1294
86	0.0320	0.1292	0.1292
87	0.0323	0.1291	0.1291
88	0.0324	0.1289	0.1289
89	0.0321	0.1293	0.1293
90	0.0325	0.1282	0.1282
91	0.0323	0.1281	0.1281
92	0.0320	0.1285	0.1285
93	0.0318	0.1279	0.1279
94	0.0321	0.1282	0.1282
95	0.0322	0.1291	0.1291
96	0.0320	0.1283	0.1283
97	0.0324	0.1285	0.1285
98	0.0321	0.1290	0.1290
99	0.0320	0.1281	0.1281
0.0423 0.1274 0.1274
load_model: 0
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(32, 4, 4), (32, 4, 4), (32, 4, 4)]
batch_size: 200
dataset: cifar10
kernel_pool_size: 1
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.6956	0.6014	0.6014
1	0.4874	0.3870	0.3870
2	0.4006	0.3577	0.3577
3	0.3515	0.3388	0.3388
4	0.3215	0.2671	0.2671
5	0.2868	0.2789	0.2789
6	0.2620	0.2693	0.2693
7	0.2411	0.2471	0.2471
8	0.2223	0.2333	0.2333
9	0.2064	0.2373	0.2373
10	0.1939	0.2024	0.2024
11	0.1852	0.1991	0.1991
12	0.1713	0.2016	0.2016
13	0.1658	0.1679	0.1679
14	0.1569	0.1807	0.1807
15	0.1516	0.2098	0.2098
16	0.1436	0.1785	0.1785
17	0.1365	0.1756	0.1756
18	0.1361	0.1716	0.1716
19	0.1287	0.1820	0.1820
20	0.0986	0.1402	0.1402
21	0.0931	0.1490	0.1490
22	0.0914	0.1453	0.1453
23	0.0884	0.1454	0.1454
24	0.0875	0.1588	0.1588
25	0.0865	0.1392	0.1392
26	0.0841	0.1494	0.1494
27	0.0816	0.1401	0.1401
28	0.0800	0.1458	0.1458
29	0.0776	0.1387	0.1387
30	0.0761	0.1379	0.1379
31	0.0734	0.1489	0.1489
32	0.0723	0.1372	0.1372
33	0.0707	0.1380	0.1380
34	0.0678	0.1363	0.1363
35	0.0670	0.1319	0.1319
36	0.0667	0.1403	0.1403
37	0.0646	0.1395	0.1395
38	0.0641	0.1486	0.1486
39	0.0627	0.1392	0.1392
40	0.0619	0.1373	0.1373
41	0.0598	0.1433	0.1433
42	0.0472	0.1277	0.1277
43	0.0463	0.1260	0.1260
44	0.0437	0.1267	0.1267
45	0.0430	0.1312	0.1312
46	0.0428	0.1315	0.1315
47	0.0416	0.1348	0.1348
48	0.0407	0.1260	0.1260
49	0.0400	0.1267	0.1267
50	0.0396	0.1249	0.1249
51	0.0389	0.1280	0.1280
52	0.0373	0.1271	0.1271
53	0.0369	0.1272	0.1272
54	0.0360	0.1270	0.1270
55	0.0350	0.1286	0.1286
56	0.0352	0.1332	0.1332
57	0.0298	0.1221	0.1221
58	0.0282	0.1234	0.1234
59	0.0278	0.1227	0.1227
60	0.0272	0.1210	0.1210
61	0.0273	0.1225	0.1225
62	0.0261	0.1231	0.1231
63	0.0266	0.1230	0.1230
64	0.0262	0.1212	0.1212
65	0.0254	0.1229	0.1229
66	0.0253	0.1199	0.1199
67	0.0252	0.1239	0.1239
68	0.0239	0.1212	0.1212
69	0.0253	0.1216	0.1216
70	0.0246	0.1247	0.1247
71	0.0242	0.1210	0.1210
72	0.0238	0.1210	0.1210
73	0.0213	0.1206	0.1206
74	0.0206	0.1196	0.1196
75	0.0203	0.1190	0.1190
76	0.0205	0.1207	0.1207
77	0.0202	0.1218	0.1218
78	0.0192	0.1209	0.1209
79	0.0199	0.1213	0.1213
80	0.0199	0.1192	0.1192
81	0.0190	0.1204	0.1204
82	0.0189	0.1211	0.1211
83	0.0185	0.1202	0.1202
84	0.0175	0.1203	0.1203
85	0.0184	0.1203	0.1203
86	0.0183	0.1200	0.1200
87	0.0182	0.1215	0.1215
88	0.0177	0.1202	0.1202
89	0.0176	0.1208	0.1208
90	0.0176	0.1202	0.1202
91	0.0171	0.1187	0.1187
92	0.0174	0.1203	0.1203
93	0.0176	0.1188	0.1188
94	0.0171	0.1178	0.1178
95	0.0170	0.1198	0.1198
96	0.0172	0.1185	0.1185
97	0.0163	0.1183	0.1183
98	0.0170	0.1202	0.1202
99	0.0168	0.1194	0.1194
0.0171 0.1178 0.1178
load_model: 0
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(64, 4, 4), (64, 4, 4), (64, 4, 4)]
batch_size: 200
dataset: cifar10
kernel_pool_size: 2
patience: 5
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7232	0.5670	0.5670
1	0.5292	0.4354	0.4354
2	0.4188	0.3656	0.3656
3	0.3537	0.3550	0.3550
4	0.3121	0.3252	0.3252
5	0.2814	0.2741	0.2741
6	0.2544	0.2801	0.2801
7	0.2354	0.3010	0.3010
8	0.2211	0.2323	0.2323
9	0.2054	0.2138	0.2138
10	0.1942	0.2116	0.2116
11	0.1886	0.1988	0.1988
12	0.1774	0.1993	0.1993
13	0.1743	0.1866	0.1866
14	0.1675	0.2023	0.2023
15	0.1616	0.2001	0.2001
16	0.1540	0.1902	0.1902
17	0.1487	0.1814	0.1814
18	0.1435	0.1906	0.1906
19	0.1417	0.1765	0.1765
20	0.1352	0.2098	0.2098
21	0.1292	0.1686	0.1686
22	0.1323	0.1840	0.1840
23	0.1261	0.1727	0.1727
24	0.1218	0.1675	0.1675
25	0.1181	0.1601	0.1601
26	0.1149	0.1803	0.1803
27	0.1130	0.1659	0.1659
28	0.1107	0.1847	0.1847
29	0.1084	0.1563	0.1563
30	0.1049	0.1556	0.1556
31	0.1035	0.1510	0.1510
32	0.0993	0.1655	0.1655
33	0.0975	0.1406	0.1406
34	0.0960	0.1460	0.1460
35	0.0971	0.1491	0.1491
36	0.0940	0.1577	0.1577
37	0.0917	0.1383	0.1383
38	0.0893	0.1584	0.1584
39	0.0866	0.1660	0.1660
40	0.0879	0.1586	0.1586
41	0.0851	0.1432	0.1432
42	0.0824	0.1491	0.1491
43	0.0837	0.1502	0.1502
44	0.0665	0.1359	0.1359
45	0.0629	0.1327	0.1327
46	0.0606	0.1282	0.1282
47	0.0606	0.1230	0.1230
48	0.0585	0.1415	0.1415
49	0.0583	0.1323	0.1323
50	0.0558	0.1307	0.1307
51	0.0538	0.1299	0.1299
52	0.0559	0.1283	0.1283
53	0.0541	0.1353	0.1353
54	0.0467	0.1233	0.1233
55	0.0453	0.1232	0.1232
56	0.0439	0.1236	0.1236
57	0.0425	0.1264	0.1264
58	0.0425	0.1243	0.1243
59	0.0430	0.1242	0.1242
60	0.0384	0.1225	0.1225
61	0.0373	0.1223	0.1223
62	0.0364	0.1246	0.1246
63	0.0358	0.1219	0.1219
64	0.0360	0.1206	0.1206
65	0.0354	0.1221	0.1221
66	0.0359	0.1258	0.1258
67	0.0358	0.1191	0.1191
68	0.0345	0.1209	0.1209
69	0.0340	0.1200	0.1200
70	0.0336	0.1200	0.1200
71	0.0338	0.1220	0.1220
72	0.0335	0.1204	0.1204
73	0.0336	0.1224	0.1224
74	0.0317	0.1197	0.1197
75	0.0316	0.1214	0.1214
76	0.0311	0.1205	0.1205
77	0.0307	0.1172	0.1172
78	0.0308	0.1188	0.1188
79	0.0301	0.1193	0.1193
80	0.0300	0.1216	0.1216
81	0.0301	0.1187	0.1187
82	0.0300	0.1173	0.1173
83	0.0301	0.1175	0.1175
84	0.0291	0.1186	0.1186
85	0.0289	0.1188	0.1188
86	0.0286	0.1186	0.1186
87	0.0286	0.1178	0.1178
88	0.0282	0.1170	0.1170
89	0.0284	0.1177	0.1177
90	0.0281	0.1169	0.1169
91	0.0279	0.1177	0.1177
92	0.0276	0.1168	0.1168
93	0.0277	0.1182	0.1182
94	0.0277	0.1183	0.1183
95	0.0280	0.1172	0.1172
96	0.0271	0.1173	0.1173
97	0.0274	0.1174	0.1174
98	0.0277	0.1175	0.1175
99	0.0274	0.1163	0.1163
0.0274 0.1163 0.1163
load_model: 0
train_epochs: 200
save_model: model_saved.npz
filter_shape: [(32, 4, 4), (32, 4, 4), (32, 4, 4)]
batch_size: 200
dataset: cifar10
kernel_pool_size: 1
patience: 10
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.6954	0.5589	0.5589
1	0.4991	0.5546	0.5546
2	0.4235	0.4490	0.4490
3	0.3662	0.3173	0.3173
4	0.3261	0.3261	0.3261
5	0.2904	0.2555	0.2555
6	0.2662	0.2683	0.2683
7	0.2477	0.2507	0.2507
8	0.2253	0.2389	0.2389
9	0.2088	0.2236	0.2236
10	0.1998	0.2538	0.2538
11	0.1867	0.1971	0.1971
12	0.1736	0.2085	0.2085
13	0.1661	0.2047	0.2047
14	0.1565	0.1889	0.1889
15	0.1515	0.1859	0.1859
16	0.1472	0.1932	0.1932
17	0.1395	0.1773	0.1773
18	0.1321	0.1598	0.1598
19	0.1290	0.1697	0.1697
20	0.1249	0.1588	0.1588
21	0.1211	0.1718	0.1718
22	0.1184	0.1761	0.1761
23	0.1124	0.1586	0.1586
24	0.1069	0.1515	0.1515
25	0.1078	0.1639	0.1639
26	0.1044	0.1500	0.1500
27	0.0998	0.1718	0.1718
28	0.0969	0.1586	0.1586
29	0.0943	0.1474	0.1474
30	0.0934	0.1534	0.1534
31	0.0883	0.1581	0.1581
32	0.0881	0.1496	0.1496
33	0.0861	0.1434	0.1434
34	0.0866	0.1480	0.1480
35	0.0806	0.1518	0.1518
36	0.0796	0.1528	0.1528
37	0.0776	0.1424	0.1424
38	0.0746	0.1466	0.1466
39	0.0734	0.1533	0.1533
40	0.0715	0.1310	0.1310
41	0.0708	0.1371	0.1371
42	0.0669	0.1418	0.1418
43	0.0688	0.1299	0.1299
44	0.0652	0.1277	0.1277
45	0.0647	0.1426	0.1426
46	0.0638	0.1350	0.1350
47	0.0628	0.1345	0.1345
48	0.0627	0.1354	0.1354
49	0.0583	0.1379	0.1379
50	0.0595	0.1326	0.1326
51	0.0575	0.1373	0.1373
52	0.0561	0.1400	0.1400
53	0.0553	0.1481	0.1481
54	0.0526	0.1341	0.1341
55	0.0543	0.1381	0.1381
56	0.0370	0.1247	0.1247
57	0.0347	0.1264	0.1264
58	0.0342	0.1258	0.1258
59	0.0319	0.1222	0.1222
60	0.0321	0.1265	0.1265
61	0.0311	0.1267	0.1267
62	0.0306	0.1250	0.1250
63	0.0290	0.1205	0.1205
64	0.0297	0.1205	0.1205
65	0.0294	0.1221	0.1221
66	0.0275	0.1261	0.1261
67	0.0274	0.1280	0.1280
68	0.0279	0.1183	0.1183
69	0.0260	0.1252	0.1252
70	0.0258	0.1258	0.1258
71	0.0261	0.1231	0.1231
72	0.0249	0.1251	0.1251
73	0.0245	0.1236	0.1236
74	0.0243	0.1226	0.1226
75	0.0239	0.1192	0.1192
76	0.0242	0.1239	0.1239
77	0.0237	0.1255	0.1255
78	0.0221	0.1210	0.1210
79	0.0221	0.1188	0.1188
80	0.0166	0.1145	0.1145
81	0.0159	0.1150	0.1150
82	0.0156	0.1210	0.1210
83	0.0153	0.1183	0.1183
84	0.0153	0.1172	0.1172
85	0.0147	0.1179	0.1179
86	0.0148	0.1150	0.1150
87	0.0136	0.1180	0.1180
88	0.0136	0.1154	0.1154
89	0.0130	0.1132	0.1132
90	0.0128	0.1171	0.1171
91	0.0133	0.1192	0.1192
92	0.0124	0.1171	0.1171
93	0.0121	0.1174	0.1174
94	0.0124	0.1197	0.1197
95	0.0120	0.1205	0.1205
96	0.0125	0.1159	0.1159
97	0.0118	0.1218	0.1218
98	0.0120	0.1215	0.1215
99	0.0119	0.1177	0.1177
100	0.0120	0.1176	0.1176
101	0.0101	0.1161	0.1161
102	0.0093	0.1148	0.1148
103	0.0090	0.1161	0.1161
104	0.0095	0.1187	0.1187
105	0.0090	0.1158	0.1158
106	0.0084	0.1169	0.1169
107	0.0082	0.1166	0.1166
108	0.0085	0.1158	0.1158
109	0.0088	0.1155	0.1155
110	0.0084	0.1180	0.1180
111	0.0078	0.1160	0.1160
112	0.0073	0.1141	0.1141
113	0.0070	0.1172	0.1172
114	0.0072	0.1140	0.1140
115	0.0071	0.1159	0.1159
116	0.0072	0.1155	0.1155
117	0.0071	0.1154	0.1154
118	0.0069	0.1159	0.1159
119	0.0065	0.1148	0.1148
120	0.0064	0.1146	0.1146
121	0.0068	0.1144	0.1144
122	0.0068	0.1143	0.1143
123	0.0062	0.1154	0.1154
124	0.0062	0.1146	0.1146
125	0.0058	0.1138	0.1138
126	0.0057	0.1145	0.1145
127	0.0059	0.1142	0.1142
128	0.0058	0.1146	0.1146
129	0.0055	0.1151	0.1151
130	0.0058	0.1154	0.1154
131	0.0057	0.1149	0.1149
132	0.0057	0.1136	0.1136
133	0.0056	0.1152	0.1152
134	0.0057	0.1155	0.1155
135	0.0057	0.1139	0.1139
136	0.0057	0.1132	0.1132
137	0.0056	0.1149	0.1149
138	0.0056	0.1141	0.1141
139	0.0055	0.1141	0.1141
140	0.0056	0.1129	0.1129
141	0.0054	0.1146	0.1146
142	0.0057	0.1140	0.1140
143	0.0058	0.1148	0.1148
144	0.0052	0.1139	0.1139
145	0.0052	0.1145	0.1145
146	0.0050	0.1138	0.1138
147	0.0053	0.1140	0.1140
148	0.0054	0.1147	0.1147
149	0.0051	0.1143	0.1143
150	0.0054	0.1129	0.1129
151	0.0052	0.1150	0.1150
152	0.0053	0.1154	0.1154
153	0.0054	0.1133	0.1133
154	0.0052	0.1150	0.1150
155	0.0052	0.1140	0.1140
156	0.0054	0.1141	0.1141
157	0.0053	0.1145	0.1145
158	0.0053	0.1128	0.1128
159	0.0052	0.1144	0.1144
160	0.0052	0.1146	0.1146
161	0.0050	0.1144	0.1144
162	0.0051	0.1154	0.1154
163	0.0049	0.1133	0.1133
164	0.0050	0.1143	0.1143
165	0.0051	0.1150	0.1150
166	0.0050	0.1135	0.1135
167	0.0049	0.1138	0.1138
168	0.0049	0.1136	0.1136
169	0.0048	0.1139	0.1139
170	0.0050	0.1145	0.1145
171	0.0049	0.1138	0.1138
172	0.0049	0.1139	0.1139
173	0.0050	0.1144	0.1144
174	0.0050	0.1149	0.1149
175	0.0049	0.1139	0.1139
176	0.0048	0.1141	0.1141
177	0.0050	0.1146	0.1146
178	0.0047	0.1143	0.1143
179	0.0049	0.1149	0.1149
180	0.0050	0.1142	0.1142
181	0.0047	0.1144	0.1144
182	0.0046	0.1148	0.1148
183	0.0049	0.1152	0.1152
184	0.0048	0.1139	0.1139
185	0.0049	0.1136	0.1136
186	0.0048	0.1142	0.1142
187	0.0048	0.1145	0.1145
188	0.0050	0.1146	0.1146
189	0.0050	0.1145	0.1145
190	0.0048	0.1140	0.1140
191	0.0049	0.1144	0.1144
192	0.0049	0.1142	0.1142
193	0.0051	0.1150	0.1150
194	0.0049	0.1140	0.1140
195	0.0051	0.1140	0.1140
196	0.0048	0.1141	0.1141
197	0.0050	0.1142	0.1142
198	0.0049	0.1140	0.1140
199	0.0049	0.1140	0.1140
0.0053 0.1128 0.1128
load_model: 0
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(64, 4, 4), (64, 4, 4), (64, 4, 4)]
batch_size: 200
dataset: cifar10
kernel_pool_size: 2
patience: 10
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7143	0.5743	0.5743
1	0.5060	0.3990	0.3990
2	0.4107	0.4108	0.4108
3	0.3564	0.3699	0.3699
4	0.3099	0.3209	0.3209
5	0.2838	0.2796	0.2796
6	0.2526	0.2455	0.2455
7	0.2301	0.2501	0.2501
8	0.2151	0.2182	0.2182
9	0.2008	0.2456	0.2456
10	0.1925	0.2117	0.2117
11	0.1828	0.1902	0.1902
12	0.1751	0.2122	0.2122
13	0.1685	0.2267	0.2267
14	0.1611	0.1848	0.1848
15	0.1541	0.1936	0.1936
16	0.1492	0.1853	0.1853
17	0.1441	0.1660	0.1660
18	0.1389	0.1660	0.1660
19	0.1346	0.1696	0.1696
20	0.1299	0.1732	0.1732
21	0.1266	0.1694	0.1694
22	0.1243	0.1708	0.1708
23	0.1209	0.1615	0.1615
24	0.1158	0.1632	0.1632
25	0.1142	0.1762	0.1762
26	0.1128	0.1604	0.1604
27	0.1112	0.1521	0.1521
28	0.1060	0.1560	0.1560
29	0.1071	0.1584	0.1584
30	0.0990	0.1482	0.1482
31	0.0999	0.1486	0.1486
32	0.1002	0.1495	0.1495
33	0.0982	0.1485	0.1485
34	0.0955	0.1870	0.1870
35	0.0938	0.1386	0.1386
36	0.0890	0.1582	0.1582
37	0.0870	0.1547	0.1547
38	0.0871	0.1355	0.1355
39	0.0867	0.1404	0.1404
40	0.0829	0.1515	0.1515
41	0.0848	0.1527	0.1527
42	0.0812	0.1480	0.1480
43	0.0772	0.1463	0.1463
44	0.0783	0.1506	0.1506
45	0.0774	0.1356	0.1356
46	0.0759	0.1346	0.1346
47	0.0750	0.1389	0.1389
48	0.0747	0.1331	0.1331
49	0.0717	0.1465	0.1465
50	0.0696	0.1354	0.1354
51	0.0679	0.1362	0.1362
52	0.0691	0.1337	0.1337
53	0.0679	0.1384	0.1384
54	0.0663	0.1392	0.1392
55	0.0668	0.1392	0.1392
56	0.0649	0.1385	0.1385
57	0.0624	0.1439	0.1439
58	0.0617	0.1463	0.1463
59	0.0605	0.1379	0.1379
60	0.0472	0.1316	0.1316
61	0.0460	0.1289	0.1289
62	0.0430	0.1245	0.1245
63	0.0435	0.1255	0.1255
64	0.0423	0.1257	0.1257
65	0.0408	0.1239	0.1239
66	0.0412	0.1269	0.1269
67	0.0422	0.1311	0.1311
68	0.0402	0.1195	0.1195
69	0.0389	0.1314	0.1314
70	0.0382	0.1216	0.1216
71	0.0392	0.1234	0.1234
72	0.0378	0.1266	0.1266
73	0.0378	0.1234	0.1234
74	0.0369	0.1248	0.1248
75	0.0361	0.1232	0.1232
76	0.0370	0.1275	0.1275
77	0.0361	0.1246	0.1246
78	0.0352	0.1292	0.1292
79	0.0355	0.1247	0.1247
80	0.0292	0.1185	0.1185
81	0.0271	0.1230	0.1230
82	0.0281	0.1178	0.1178
83	0.0259	0.1189	0.1189
84	0.0267	0.1220	0.1220
85	0.0260	0.1237	0.1237
86	0.0253	0.1193	0.1193
87	0.0253	0.1178	0.1178
88	0.0253	0.1206	0.1206
89	0.0246	0.1179	0.1179
90	0.0235	0.1200	0.1200
91	0.0241	0.1166	0.1166
92	0.0239	0.1246	0.1246
93	0.0248	0.1235	0.1235
94	0.0247	0.1251	0.1251
95	0.0240	0.1236	0.1236
96	0.0234	0.1216	0.1216
97	0.0237	0.1202	0.1202
98	0.0229	0.1173	0.1173
99	0.0221	0.1161	0.1161
0.0221 0.1161 0.1161
load_model: 0
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(128, 4, 4), (128, 4, 4), (128, 4, 4)]
batch_size: 200
dataset: cifar10
kernel_pool_size: 2
patience: 10
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.7060	0.5767	0.5767
1	0.4890	0.4455	0.4455
2	0.3828	0.3838	0.3838
3	0.3077	0.2982	0.2982
4	0.2572	0.2473	0.2473
5	0.2241	0.2178	0.2178
6	0.1984	0.2016	0.2016
7	0.1809	0.2087	0.2087
8	0.1639	0.1743	0.1743
9	0.1498	0.1684	0.1684
10	0.1388	0.1637	0.1637
11	0.1292	0.1609	0.1609
12	0.1208	0.1781	0.1781
13	0.1121	0.1502	0.1502
14	0.1032	0.1490	0.1490
15	0.0967	0.1427	0.1427
16	0.0897	0.1574	0.1574
17	0.0840	0.1373	0.1373
18	0.0775	0.1363	0.1363
19	0.0733	0.1518	0.1518
20	0.0716	0.1403	0.1403
21	0.0622	0.1532	0.1532
22	0.0609	0.1395	0.1395
23	0.0561	0.1326	0.1326
24	0.0511	0.1199	0.1199
25	0.0505	0.1322	0.1322
26	0.0477	0.1219	0.1219
27	0.0457	0.1266	0.1266
28	0.0403	0.1261	0.1261
29	0.0364	0.1378	0.1378
30	0.0359	0.1173	0.1173
31	0.0331	0.1203	0.1203
32	0.0311	0.1203	0.1203
33	0.0306	0.1172	0.1172
34	0.0284	0.1277	0.1277
35	0.0270	0.1169	0.1169
36	0.0248	0.1092	0.1092
37	0.0235	0.1237	0.1237
38	0.0223	0.1172	0.1172
39	0.0212	0.1146	0.1146
40	0.0209	0.1153	0.1153
41	0.0174	0.1197	0.1197
42	0.0167	0.1090	0.1090
43	0.0163	0.1224	0.1224
44	0.0143	0.1144	0.1144
45	0.0123	0.1080	0.1080
46	0.0135	0.1244	0.1244
47	0.0129	0.1187	0.1187
48	0.0120	0.1069	0.1069
49	0.0125	0.1126	0.1126
50	0.0114	0.1165	0.1165
51	0.0102	0.1191	0.1191
52	0.0103	0.1134	0.1134
53	0.0098	0.1284	0.1284
54	0.0089	0.1167	0.1167
55	0.0086	0.1071	0.1071
56	0.0080	0.1078	0.1078
57	0.0072	0.1146	0.1146
58	0.0076	0.1080	0.1080
59	0.0072	0.1059	0.1059
60	0.0063	0.1079	0.1079
61	0.0059	0.1103	0.1103
62	0.0055	0.1106	0.1106
63	0.0057	0.1003	0.1003
64	0.0052	0.1060	0.1060
65	0.0051	0.1055	0.1055
66	0.0045	0.1068	0.1068
67	0.0046	0.1051	0.1051
68	0.0048	0.1032	0.1032
69	0.0037	0.0991	0.0991
70	0.0037	0.1048	0.1048
71	0.0035	0.1012	0.1012
72	0.0038	0.1024	0.1024
73	0.0036	0.1072	0.1072
74	0.0031	0.1019	0.1019
75	0.0033	0.1119	0.1119
76	0.0027	0.1038	0.1038
77	0.0032	0.1011	0.1011
78	0.0026	0.1003	0.1003
79	0.0026	0.1042	0.1042
80	0.0027	0.1030	0.1030
81	0.0007	0.0990	0.0990
82	0.0006	0.0965	0.0965
83	0.0004	0.0951	0.0951
84	0.0004	0.0940	0.0940
85	0.0002	0.0945	0.0945
86	0.0003	0.0915	0.0915
87	0.0001	0.0938	0.0938
88	0.0002	0.0956	0.0956
89	0.0001	0.0985	0.0985
90	0.0002	0.0971	0.0971
91	0.0002	0.0958	0.0958
92	0.0001	0.0978	0.0978
93	0.0003	0.0957	0.0957
94	0.0002	0.0937	0.0937
95	0.0002	0.0948	0.0948
96	0.0001	0.0956	0.0956
97	0.0002	0.0939	0.0939
98	0.0000	0.0934	0.0934
99	0.0000	0.0923	0.0923
0.0003 0.0915 0.0915
load_model: 0
train_epochs: 5
save_model: model_saved.npz
filter_shape: [(64, 6, 6), (4, 4), (64, 6, 6), (4, 4)]
batch_size: 200
dataset: cifar10
kernel_pool_size: -1
patience: 10
lr: 1.0
doubleconv: 1
kernel_size: 5
train_on_valid: 1
0	0.6239	0.5426	0.5426
1	0.4819	0.4344	0.4344
2	0.4290	0.4439	0.4439
3	0.3978	0.3850	0.3850
4	0.3717	0.3644	0.3644
0.3717 0.3644 0.3644
load_model: 0
train_epochs: 100
save_model: model_saved.npz
filter_shape: [(196, 4, 4), (196, 4, 4), (196, 4, 4)]
batch_size: 200
dataset: cifar10
kernel_pool_size: 2
patience: 10
lr: 1.0
std_p: 0.5
doubleconv: 1
pool_size: 2
n_nested: 2
train_on_valid: 1
0	0.6782	0.5270	0.5270
1	0.4569	0.4838	0.4838
2	0.3647	0.3284	0.3284
3	0.2879	0.2625	0.2625
4	0.2429	0.2170	0.2170
5	0.2114	0.2242	0.2242
6	0.1801	0.2002	0.2002
7	0.1626	0.1781	0.1781
8	0.1463	0.1907	0.1907
9	0.1303	0.1630	0.1630
10	0.1208	0.1852	0.1852
11	0.1082	0.1392	0.1392
12	0.0968	0.1330	0.1330
13	0.0876	0.1590	0.1590
14	0.0800	0.1730	0.1730
15	0.0713	0.1477	0.1477
16	0.0635	0.1513	0.1513
17	0.0586	0.1172	0.1172
18	0.0541	0.1343	0.1343
19	0.0487	0.1199	0.1199
20	0.0416	0.1210	0.1210
21	0.0387	0.1196	0.1196
22	0.0356	0.1238	0.1238
23	0.0295	0.1542	0.1542
24	0.0289	0.1142	0.1142
25	0.0244	0.1216	0.1216
26	0.0235	0.1320	0.1320
27	0.0211	0.1301	0.1301
28	0.0178	0.1147	0.1147
29	0.0173	0.1087	0.1087
30	0.0155	0.1127	0.1127
31	0.0128	0.1057	0.1057
32	0.0120	0.1073	0.1073
33	0.0104	0.1058	0.1058
34	0.0097	0.1174	0.1174
35	0.0108	0.1101	0.1101
36	0.0080	0.1236	0.1236
37	0.0078	0.1149	0.1149
38	0.0072	0.1072	0.1072
39	0.0066	0.1106	0.1106
40	0.0053	0.1113	0.1113
41	0.0056	0.1031	0.1031
42	0.0046	0.1133	0.1133
43	0.0041	0.1042	0.1042
44	0.0043	0.1047	0.1047
45	0.0036	0.1059	0.1059
46	0.0034	0.1056	0.1056
47	0.0037	0.1053	0.1053
48	0.0029	0.1111	0.1111
49	0.0033	0.1084	0.1084
50	0.0027	0.1146	0.1146
51	0.0034	0.1094	0.1094
52	0.0019	0.1051	0.1051
53	0.0003	0.0940	0.0940
54	0.0001	0.0984	0.0984
55	0.0001	0.0946	0.0946
56	0.0000	0.0940	0.0940
57	0.0001	0.0973	0.0973
58	0.0000	0.0938	0.0938
59	0.0000	0.0937	0.0937
60	0.0000	0.0969	0.0969
61	0.0001	0.0948	0.0948
62	0.0000	0.0936	0.0936
63	0.0000	0.0962	0.0962
64	0.0000	0.0937	0.0937
65	0.0000	0.0962	0.0962
66	0.0000	0.0927	0.0927
67	0.0000	0.0924	0.0924
68	0.0000	0.0929	0.0929
69	0.0000	0.0921	0.0921
70	0.0000	0.0951	0.0951
71	0.0000	0.0971	0.0971
72	0.0000	0.0933	0.0933
73	0.0001	0.0958	0.0958
74	0.0000	0.0937	0.0937
75	0.0000	0.0950	0.0950
76	0.0000	0.0950	0.0950
77	0.0000	0.0967	0.0967
78	0.0000	0.0972	0.0972
79	0.0000	0.0944	0.0944
80	0.0000	0.0956	0.0956
81	0.0000	0.0895	0.0895
82	0.0000	0.0893	0.0893
83	0.0000	0.0932	0.0932
84	0.0000	0.0915	0.0915
85	0.0000	0.0917	0.0917
86	0.0000	0.0886	0.0886
87	0.0000	0.0909	0.0909
88	0.0000	0.0906	0.0906
89	0.0000	0.0905	0.0905
90	0.0000	0.0921	0.0921
91	0.0000	0.0923	0.0923
92	0.0000	0.0917	0.0917
93	0.0000	0.0903	0.0903
94	0.0000	0.0921	0.0921
95	0.0000	0.0897	0.0897
96	0.0000	0.0893	0.0893
97	0.0000	0.0911	0.0911
98	0.0000	0.0903	0.0903
99	0.0000	0.0917	0.0917
0.0000 0.0886 0.0886
